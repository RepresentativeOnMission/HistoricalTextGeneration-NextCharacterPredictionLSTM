{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e8dfe2b-4149-4b10-8114-5403aa77627e",
      "metadata": {
        "id": "1e8dfe2b-4149-4b10-8114-5403aa77627e"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "837a5b60-6479-4dcd-8da0-e3b28aeafbc3",
      "metadata": {
        "id": "837a5b60-6479-4dcd-8da0-e3b28aeafbc3",
        "outputId": "4250a5ef-6d3e-4bc9-b416-940e9d0d869a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA GeForce GTX 960\n"
          ]
        }
      ],
      "source": [
        "#Load the device GPU\n",
        "gpu = torch.device(\"cuda:0\")\n",
        "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20114766-5d9c-4d14-a1d9-ffe2b1d52f81",
      "metadata": {
        "tags": [],
        "id": "20114766-5d9c-4d14-a1d9-ffe2b1d52f81"
      },
      "outputs": [],
      "source": [
        "#load the dataset\n",
        "import codecs\n",
        "dataset_t = \"\"\n",
        "with codecs.open('dataset/dataset_barbero_sarzana.txt', encoding='utf-8') as f:\n",
        "    for character in f:\n",
        "        dataset_t = dataset_t + character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f58b2b63-a5ee-4522-8e3b-e0672a71f2f5",
      "metadata": {
        "tags": [],
        "id": "f58b2b63-a5ee-4522-8e3b-e0672a71f2f5",
        "outputId": "0ef9df83-7f1e-4a57-b8be-5e29ab739748"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\grand\\AppData\\Local\\Temp\\ipykernel_29328\\1033458228.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dictionary['int_encoding'] = label_enc.fit_transform(dictionary['data'])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [1., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# transform dataset from numeric to one-hot\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "whole_dataset_list_of_chars = list(dataset_t)\n",
        "dataset_int = pd.DataFrame(whole_dataset_list_of_chars, columns=['data'])\n",
        "\n",
        "# create a dictionary\n",
        "label_enc = LabelEncoder()\n",
        "dictionary = dataset_int.drop_duplicates(subset=['data'])\n",
        "dictionary['int_encoding'] = label_enc.fit_transform(dictionary['data'])\n",
        "dataset_int['int_encoding'] = label_enc.fit_transform(dataset_int['data'])\n",
        "\n",
        "\n",
        "# one hot encode\n",
        "one_hot_enc = OneHotEncoder()\n",
        "# lstm uses float32 insted of float64.\n",
        "one_hot_encoded_dataset = one_hot_enc.fit_transform(dataset_int[['int_encoding']]).toarray().astype(np.float32)\n",
        "one_hot_encoded_dataset = torch.from_numpy(one_hot_encoded_dataset)\n",
        "one_hot_encoded_dataset = one_hot_encoded_dataset.to(gpu)\n",
        "print(one_hot_encoded_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6b005c-535e-455d-9ed7-12562d84dbde",
      "metadata": {
        "id": "3c6b005c-535e-455d-9ed7-12562d84dbde",
        "outputId": "0659d3f7-f30a-46fe-a336-40e3d52ece54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        data  int_encoding\n",
            "0          b            39\n",
            "1          e            42\n",
            "2          n            51\n",
            "4                        1\n",
            "6          u            58\n",
            "...      ...           ...\n",
            "1099082    -             4\n",
            "1099618    U            33\n",
            "1100751    Ãˆ            64\n",
            "1101278    N            26\n",
            "1104174    V            34\n",
            "\n",
            "[72 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "print(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7dc00c6-f9fe-45fc-ae9d-fa7529651a71",
      "metadata": {
        "id": "d7dc00c6-f9fe-45fc-ae9d-fa7529651a71"
      },
      "outputs": [],
      "source": [
        "# check that the device set is the GPU\n",
        "assert one_hot_encoded_dataset.get_device() == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d28b0528-4984-4602-a237-29bdd620ffa5",
      "metadata": {
        "id": "d28b0528-4984-4602-a237-29bdd620ffa5",
        "outputId": "539bbfcc-a5ab-401a-d7f5-048449290061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22736\n"
          ]
        }
      ],
      "source": [
        "# ADD LABELS: turn dataset from a string \"s1,s2,...,si,si+1\" to ((s1,...,sk),(s2,...,sk+1)),...\n",
        "k = 100\n",
        "dataset = []\n",
        "for i in range(0,len(one_hot_encoded_dataset)-k,int(k/2)):\n",
        "    x = []\n",
        "    y = []\n",
        "    for j in range(k):\n",
        "        x.append(one_hot_encoded_dataset[i+j])\n",
        "        y.append(one_hot_encoded_dataset[i+j+1])\n",
        "    dataset.append((x, y))\n",
        "print(len(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a7355c-4d10-44dc-8138-3382c47e0c1a",
      "metadata": {
        "id": "c1a7355c-4d10-44dc-8138-3382c47e0c1a",
        "outputId": "4f51ce79-8ec3-4fa5-cdda-1903c690c713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(len(dataset[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8252614-b9f2-4b8c-9485-fcdca17916e0",
      "metadata": {
        "tags": [],
        "id": "a8252614-b9f2-4b8c-9485-fcdca17916e0"
      },
      "outputs": [],
      "source": [
        "#create the dataloader: 70% for training, 30% for test\n",
        "train_set_threshold = int(len(dataset) * 0.7)\n",
        "training_set = dataset[:train_set_threshold]\n",
        "test_set = dataset[train_set_threshold+1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6b3684c-ee3b-4e97-8c2a-3188611969a3",
      "metadata": {
        "tags": [],
        "id": "d6b3684c-ee3b-4e97-8c2a-3188611969a3"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_dataloader = DataLoader(training_set, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_set, batch_size=1, shuffle=True)\n",
        "iterator = iter(train_dataloader)\n",
        "x, y = next(iter(iterator))\n",
        "# check that the data is in the form: L,N,H. L=sequence length, N=batch size, H=input size\n",
        "assert len(x) == k and len(x[0]) == batch_size and len(x[0][0]) == 72\n",
        "assert len(y) == k and len(y[0]) == batch_size and len(y[0][0]) == 72\n",
        "#for x,y in iterator:\n",
        "#    print(x)\n",
        "#    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "586fe12b-820e-49e5-bc0a-d826cba8e0ed",
      "metadata": {
        "tags": [],
        "id": "586fe12b-820e-49e5-bc0a-d826cba8e0ed",
        "outputId": "5adbf41a-3fc2-4d52-b2c6-daf0ce9cec24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72\n"
          ]
        }
      ],
      "source": [
        "#build architecture\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        # weights and biases are initialized using the uniform distribution:\n",
        "        #      weights.uniform_(-sqrt(output_size), +sqrt(output_size))\n",
        "        self.lstm_layer = torch.nn.LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
        "        self.dense = torch.nn.Linear(in_features=hidden_size, out_features=output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        # we convert a list of tensors (given as such by the dataloader) to a tensor\n",
        "        x = torch.stack(x)\n",
        "\n",
        "        # the split function collapses the batch dimension, so now, instead of having (sequence_len,batch_size,input_size) inputs,\n",
        "        # you have a number equal to batch_size of (batch_size,input_size) inputs; you will feed these inputs one after another to the lstm, using\n",
        "        # the history h_t and short term memory c_t, along with the i-th input of type (batch_size,input_size)\n",
        "        for time_step in torch.split(x, split_size_or_sections=1, dim=0):\n",
        "            # this is needed to remove a dimension that wasn't removed by split\n",
        "            time_step = torch.squeeze(time_step)\n",
        "            h_t, c_t = self.lstm_layer(time_step)\n",
        "            output = self.dense(h_t)\n",
        "            outputs.append(output)\n",
        "        # len(outputs) = batch_size. We convert outputs to a tensor by concatenating all tensors inside it\n",
        "        #outputs = torch.stack(outputs)\n",
        "        #print(outputs.size())\n",
        "        return outputs\n",
        "assert len(training_set[0][0][0]) == 72\n",
        "input_size = len(training_set[0][0][0])\n",
        "output_size = input_size\n",
        "model = NeuralNetwork(input_size, 32, output_size)\n",
        "model.to(gpu)\n",
        "print(input_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39477292-cf1a-443d-b54d-c76e308579c0",
      "metadata": {
        "tags": [],
        "id": "39477292-cf1a-443d-b54d-c76e308579c0",
        "outputId": "85c9013c-15ff-405f-c601-75f2aac4409e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "torch.Size([128, 72])\n",
            "torch.Size([100, 128, 72])\n"
          ]
        }
      ],
      "source": [
        "# try the model\n",
        "inp = next(iter(train_dataloader))[0]\n",
        "output = model(inp)\n",
        "print(len(output))\n",
        "print(output[1].size())\n",
        "#assert output.size() == (100,128,72)\n",
        "print(torch.stack(output).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef906656-38b1-474d-bf34-361490ba4066",
      "metadata": {
        "id": "ef906656-38b1-474d-bf34-361490ba4066"
      },
      "outputs": [],
      "source": [
        "#declare loss, optimizer\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448f29ec-1a00-44c4-a1b6-20839348436c",
      "metadata": {
        "id": "448f29ec-1a00-44c4-a1b6-20839348436c"
      },
      "outputs": [],
      "source": [
        "#train\n",
        "def train_cycle(EPOCHS, train_dataloader, model, loss, optimizer):\n",
        "    losses = []\n",
        "    for i in range(EPOCHS):\n",
        "        iterator = iter(train_dataloader)\n",
        "        print(\"Epoch: \" + str(i))\n",
        "        for data, label in iterator:\n",
        "            # pytorch accumulate gradients at every batch, doing this you reset these gradients\n",
        "            optimizer.zero_grad()\n",
        "            #forward step\n",
        "            outputs = model(data)\n",
        "            loss_acc = 0\n",
        "            for i in range(len(outputs)):\n",
        "                loss_value = loss(outputs[i],label[i])\n",
        "                #backward step: compute gradients (apply automatic differentiation)\n",
        "                loss_value.backward()\n",
        "                loss_acc = loss_acc + loss_value\n",
        "            loss_acc = loss_acc / len(outputs)\n",
        "            losses.append(loss_acc)\n",
        "            #update the parameters using the already computed gradients\n",
        "            optimizer.step()\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7392af7-d2df-47ce-befb-40925d96eea0",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "b7392af7-d2df-47ce-befb-40925d96eea0",
        "outputId": "d89fb0a8-3bdc-498c-eed1-33a5151a81be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Epoch: 1\n",
            "Epoch: 2\n",
            "Epoch: 3\n",
            "Epoch: 4\n",
            "Time passed: 46.121175050735474\n",
            "[tensor(4.2936, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2880, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2848, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2821, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2792, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2738, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2704, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2692, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2651, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2619, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2595, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2549, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2525, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2505, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2428, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2419, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2371, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2359, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2312, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2264, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2245, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2216, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2149, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2113, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2051, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.2038, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1971, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1951, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1882, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1852, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1824, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1715, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1688, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1644, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1599, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1560, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1501, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1421, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1368, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1307, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1276, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1193, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1145, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1078, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.1005, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0972, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0898, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0824, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0763, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0733, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0616, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0549, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0492, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0401, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0296, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0236, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0146, device='cuda:0', grad_fn=<DivBackward0>), tensor(4.0078, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9969, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9916, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9848, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9736, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9651, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9510, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9451, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9317, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9236, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9189, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.9096, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.8929, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.8922, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.8786, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.8613, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.8542, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.8402, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.8287, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.8225, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.8129, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.7986, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.7885, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.7838, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.7684, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.7566, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.7386, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.7350, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.7104, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.6967, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.6901, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.6845, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.6663, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.6549, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.6467, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.6297, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.6175, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.6053, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.5974, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.5765, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.5752, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.5501, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.5428, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.5329, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.5166, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.5041, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.4957, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.4942, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.4785, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.4616, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.4450, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.4333, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.4331, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.4264, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.4034, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.3906, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.3710, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.3631, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.3540, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.3479, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.3352, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.3163, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.3136, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.2965, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.2883, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.2690, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.2791, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.2695, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.2512, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.2337, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.2381, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.2218, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.2121, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1954, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1936, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1897, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1883, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1801, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1615, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1579, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1437, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1479, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1312, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1398, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1355, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1159, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1120, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0909, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.1085, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0972, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0991, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0756, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0837, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0920, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0907, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0889, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0751, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0525, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0610, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0434, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0429, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0211, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0431, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0291, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0313, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0076, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0148, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0143, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0134, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0047, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9995, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0002, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0004, device='cuda:0', grad_fn=<DivBackward0>), tensor(3.0104, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9909, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9938, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9918, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9914, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9648, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9749, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9747, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9809, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9659, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9695, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9784, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9737, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9690, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9651, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9741, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9684, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9593, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9247, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9456, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9320, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9334, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9597, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9541, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9133, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9385, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9367, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9082, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9193, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9246, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9089, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9308, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9248, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9027, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9247, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9487, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9335, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9246, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9095, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9300, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8944, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8848, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9080, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9162, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9073, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8969, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9143, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8927, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8890, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8907, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9020, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8766, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9123, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8757, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8828, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8887, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8777, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8785, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8771, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9140, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.9067, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8886, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8733, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8697, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8879, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8985, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8746, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8717, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8492, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8576, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8823, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8637, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8692, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8538, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8697, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8463, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8540, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8423, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8847, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8962, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8479, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8333, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8527, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8394, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8336, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8261, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8336, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8359, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8479, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8398, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8322, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8402, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8302, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8435, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8283, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8458, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8473, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8241, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8263, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8099, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8450, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8304, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8436, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8180, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8287, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8585, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8612, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8494, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8458, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8310, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8403, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8205, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8288, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7992, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8236, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8229, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8265, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7999, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8101, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8125, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8097, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8040, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8017, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8108, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8087, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8232, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8051, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8173, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8194, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8202, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7926, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8020, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8003, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8172, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8044, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8074, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8210, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8132, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8125, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8035, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8134, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8164, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8017, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7681, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7910, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7754, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7711, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.8031, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7908, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7535, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7879, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7792, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7494, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7592, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7687, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7516, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7748, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7650, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7467, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7701, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7994, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7850, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7776, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7594, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7831, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7470, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7362, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7566, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7651, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7617, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7505, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7710, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7446, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7409, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7426, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7527, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7203, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7610, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7277, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7265, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7384, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7270, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7272, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7273, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7677, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7632, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7343, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7149, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7133, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7296, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7392, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7140, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7112, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6924, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7067, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7251, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6993, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7070, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6942, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7092, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6852, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6927, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6848, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7292, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.7451, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6785, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6664, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6842, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6717, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6600, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6567, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6648, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6564, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6754, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6667, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6617, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6775, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6625, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6701, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6556, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6716, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6755, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6497, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6505, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6359, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6789, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6570, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6693, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6387, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6480, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6915, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6876, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6630, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6653, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6537, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6597, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6433, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6590, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6219, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6420, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6450, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6507, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6175, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6306, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6325, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6221, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6178, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6150, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6280, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6248, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6356, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6234, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6451, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6482, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6470, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6171, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6207, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6178, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6423, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6350, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6341, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6535, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6389, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6416, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6244, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6328, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6451, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6216, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5912, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6120, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5940, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5830, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6206, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6055, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5694, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6115, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5928, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5611, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5684, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5812, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5657, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5871, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5730, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5591, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5831, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6174, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6055, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5995, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5822, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.6014, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5671, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5599, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5752, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5812, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5868, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5751, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5989, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5649, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5622, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5609, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5729, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5324, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5756, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5494, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5395, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5585, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5493, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5478, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5491, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5957, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5931, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5481, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5276, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5296, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5418, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5522, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5255, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5214, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5121, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5323, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5426, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5100, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5231, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5110, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5249, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5029, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5126, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5109, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5536, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5767, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4906, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4812, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4943, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4884, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4712, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4735, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4799, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4659, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4888, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4807, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4838, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5091, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4875, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4877, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4778, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4903, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5032, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4727, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4709, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4632, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5135, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4863, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5001, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4614, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4717, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5290, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5191, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4829, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4939, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4829, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4886, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4816, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5060, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4556, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4748, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4829, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4929, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4526, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4706, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4753, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4574, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4543, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4525, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4705, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4650, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4723, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4718, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5020, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5065, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5035, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4723, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4699, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4672, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5028, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5036, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4994, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5272, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5027, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5134, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4843, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4940, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.5148, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4858, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4588, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4776, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4579, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4416, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4845, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4703, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4336, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4860, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4587, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4282, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4308, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4467, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4366, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4567, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4409, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4311, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4548, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4935, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4848, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4815, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4667, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4808, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4505, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4486, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4588, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4609, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4773, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4654, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4934, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4544, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4524, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4475, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4613, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4184, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4609, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4397, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4257, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4497, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4432, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4426, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4431, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4972, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4961, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4398, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4188, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4221, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4353, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4468, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4166, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4121, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4107, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4355, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4418, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4046, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4210, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4088, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4221, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4026, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4161, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4172, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4614, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.4900, device='cuda:0', grad_fn=<DivBackward0>)]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "EPOCHS = 5\n",
        "losses = train_cycle(EPOCHS, train_dataloader, model, loss, optimizer)\n",
        "\n",
        "end = time.time()\n",
        "print(\"Time passed: \" + str(end - start))\n",
        "print(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b96a1b85-6029-4854-a58d-4ba6816a99b9",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "b96a1b85-6029-4854-a58d-4ba6816a99b9",
        "outputId": "cf60cc92-a962-4c30-feba-3ef52cb918db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.29, 0.2, 0.2, 0.24, 0.28, 0.32, 0.27, 0.26, 0.34, 0.22, 0.28, 0.31, 0.31, 0.19, 0.23, 0.27, 0.18, 0.32, 0.23, 0.27, 0.33, 0.29, 0.31, 0.3, 0.23, 0.28, 0.27, 0.25, 0.35, 0.3, 0.25, 0.28, 0.26, 0.29, 0.27, 0.28, 0.34, 0.31, 0.25, 0.27, 0.29, 0.36, 0.31, 0.24, 0.2, 0.19, 0.26, 0.27, 0.28, 0.26, 0.26, 0.25, 0.28, 0.25, 0.28, 0.34, 0.25, 0.28, 0.33, 0.27, 0.31, 0.28, 0.27, 0.23, 0.19, 0.23, 0.18, 0.31, 0.26, 0.2, 0.27, 0.34, 0.29, 0.22, 0.35, 0.28, 0.14, 0.26, 0.26, 0.23, 0.28, 0.29, 0.24, 0.23, 0.28, 0.24, 0.28, 0.2, 0.34, 0.29, 0.29, 0.34, 0.26, 0.31, 0.24, 0.21, 0.3, 0.27, 0.31, 0.17, 0.23, 0.24, 0.2, 0.27, 0.28, 0.28, 0.29, 0.22, 0.24, 0.29, 0.25, 0.25, 0.26, 0.32, 0.31, 0.34, 0.26, 0.3, 0.29, 0.27, 0.31, 0.36, 0.27, 0.28, 0.25, 0.25, 0.3, 0.28, 0.24, 0.28, 0.24, 0.22, 0.26, 0.31, 0.21, 0.24, 0.26, 0.23, 0.29, 0.19, 0.33, 0.28, 0.33, 0.3, 0.22, 0.2, 0.25, 0.32, 0.26, 0.3, 0.24, 0.29, 0.2, 0.19, 0.24, 0.24, 0.33, 0.25, 0.23, 0.3, 0.32, 0.22, 0.21, 0.26, 0.32, 0.22, 0.24, 0.27, 0.23, 0.28, 0.27, 0.22, 0.28, 0.24, 0.26, 0.24, 0.26, 0.33, 0.3, 0.19, 0.26, 0.23, 0.3, 0.3, 0.27, 0.26, 0.29, 0.22, 0.27, 0.21, 0.26, 0.32, 0.29, 0.28, 0.37, 0.22, 0.28, 0.25, 0.29, 0.3, 0.24, 0.27, 0.21, 0.3, 0.32, 0.31, 0.26, 0.3, 0.23, 0.29, 0.22, 0.21, 0.27, 0.25, 0.28, 0.22, 0.19, 0.26, 0.28, 0.31, 0.21, 0.23, 0.28, 0.29, 0.31, 0.27, 0.32, 0.23, 0.24, 0.28, 0.27, 0.25, 0.29, 0.25, 0.31, 0.27, 0.24, 0.32, 0.23, 0.3, 0.26, 0.25, 0.27, 0.3, 0.27, 0.25, 0.33, 0.27, 0.25, 0.27, 0.34, 0.29, 0.25, 0.26, 0.25, 0.22, 0.27, 0.26, 0.32, 0.25, 0.27, 0.28, 0.24, 0.31, 0.26, 0.27, 0.29, 0.28, 0.23, 0.26, 0.3, 0.3, 0.27, 0.24, 0.32, 0.29, 0.27, 0.27, 0.27, 0.24, 0.34, 0.26, 0.24, 0.22, 0.33, 0.23, 0.2, 0.3, 0.3, 0.29, 0.21, 0.26, 0.25, 0.21, 0.2, 0.23, 0.31, 0.24, 0.28, 0.27, 0.26, 0.24, 0.3, 0.23, 0.31, 0.31, 0.25, 0.31, 0.2, 0.3, 0.26, 0.35, 0.26, 0.25, 0.25, 0.3, 0.31, 0.29, 0.26, 0.25, 0.26, 0.25, 0.23, 0.25, 0.25, 0.25, 0.27, 0.27, 0.3, 0.29, 0.23, 0.23, 0.26, 0.21, 0.32, 0.25, 0.22, 0.28, 0.24, 0.19, 0.33, 0.24, 0.25, 0.21, 0.26, 0.25, 0.31, 0.32, 0.25, 0.25, 0.27, 0.17, 0.34, 0.29, 0.27, 0.23, 0.22, 0.21, 0.23, 0.26, 0.27, 0.21, 0.25, 0.24, 0.25, 0.25, 0.34, 0.23, 0.27, 0.24, 0.34, 0.21, 0.28, 0.28, 0.25, 0.25, 0.35, 0.27, 0.26, 0.28, 0.28, 0.25, 0.25, 0.28, 0.25, 0.24, 0.25, 0.27, 0.29, 0.25, 0.27, 0.22, 0.27, 0.29, 0.23, 0.31, 0.28, 0.25, 0.25, 0.31, 0.35, 0.24, 0.24, 0.31, 0.25, 0.31, 0.27, 0.3, 0.24, 0.3, 0.21, 0.25, 0.3, 0.27, 0.25, 0.27, 0.22, 0.23, 0.29, 0.25, 0.23, 0.27, 0.25, 0.25, 0.27, 0.21, 0.3, 0.26, 0.3, 0.27, 0.27, 0.18, 0.36, 0.29, 0.27, 0.23, 0.33, 0.29, 0.31, 0.31, 0.18, 0.31, 0.27, 0.32, 0.23, 0.24, 0.24, 0.32, 0.25, 0.35, 0.32, 0.28, 0.3, 0.29, 0.27, 0.22, 0.29, 0.3, 0.24, 0.28, 0.3, 0.22, 0.26, 0.19, 0.25, 0.29, 0.29, 0.32, 0.23, 0.29, 0.23, 0.31, 0.26, 0.3, 0.26, 0.26, 0.21, 0.27, 0.28, 0.3, 0.15, 0.25, 0.29, 0.21, 0.3, 0.26, 0.23, 0.33, 0.26, 0.2, 0.29, 0.26, 0.25, 0.19, 0.3, 0.29, 0.18, 0.28, 0.21, 0.27, 0.31, 0.28, 0.23, 0.26, 0.26, 0.27, 0.27, 0.22, 0.26, 0.24, 0.33, 0.32, 0.25, 0.25, 0.23, 0.25, 0.26, 0.28, 0.22, 0.25, 0.37, 0.27, 0.31, 0.3, 0.31, 0.28, 0.21, 0.26, 0.31, 0.29, 0.28, 0.25, 0.33, 0.27, 0.23, 0.2, 0.27, 0.29, 0.31, 0.17, 0.3, 0.26, 0.26, 0.22, 0.26, 0.25, 0.36, 0.3, 0.27, 0.26, 0.27, 0.23, 0.27, 0.23, 0.27, 0.27, 0.32, 0.24, 0.31, 0.29, 0.23, 0.26, 0.29, 0.31, 0.29, 0.28, 0.23, 0.29, 0.32, 0.23, 0.24, 0.27, 0.29, 0.26, 0.24, 0.25, 0.27, 0.27, 0.25, 0.2, 0.23, 0.28, 0.3, 0.25, 0.24, 0.26, 0.24, 0.22, 0.24, 0.25, 0.29, 0.31, 0.3, 0.29, 0.28, 0.21, 0.29, 0.26, 0.24, 0.3, 0.27, 0.2, 0.26, 0.22, 0.28, 0.26, 0.26, 0.26, 0.2, 0.25, 0.35, 0.27, 0.26, 0.33, 0.33, 0.33, 0.29, 0.28, 0.33, 0.31, 0.31, 0.21, 0.24, 0.24, 0.29, 0.27, 0.27, 0.26, 0.25, 0.34, 0.32, 0.25, 0.25, 0.25, 0.3, 0.29, 0.26, 0.21, 0.34, 0.19, 0.32, 0.22, 0.24, 0.29, 0.32, 0.27, 0.27, 0.25, 0.28, 0.29, 0.24, 0.29, 0.28, 0.26, 0.27, 0.22, 0.3, 0.26, 0.26, 0.24, 0.27, 0.26, 0.26, 0.22, 0.29, 0.24, 0.26, 0.24, 0.26, 0.28, 0.38, 0.26, 0.29, 0.26, 0.25, 0.31, 0.24, 0.27, 0.2, 0.26, 0.22, 0.26, 0.26, 0.27, 0.26, 0.36, 0.32, 0.29, 0.22, 0.23, 0.35, 0.21, 0.3, 0.33, 0.27, 0.29, 0.24, 0.29, 0.18, 0.35, 0.3, 0.28, 0.29, 0.26, 0.28, 0.26, 0.29, 0.3, 0.29, 0.24, 0.27, 0.22, 0.27, 0.27, 0.27, 0.31, 0.2, 0.32, 0.29, 0.27, 0.25, 0.17, 0.28, 0.27, 0.23, 0.22, 0.28, 0.22, 0.26, 0.27, 0.31, 0.27, 0.27, 0.28, 0.21, 0.3, 0.23, 0.24, 0.34, 0.29, 0.23, 0.27, 0.27, 0.25, 0.28, 0.23, 0.23, 0.24, 0.23, 0.28, 0.29, 0.33, 0.29, 0.25, 0.26, 0.29, 0.23, 0.25, 0.23, 0.34, 0.25, 0.32, 0.28, 0.25, 0.33, 0.27, 0.27, 0.25, 0.3, 0.29, 0.25, 0.29, 0.3, 0.23, 0.26, 0.3, 0.25, 0.24, 0.32, 0.28, 0.16, 0.24, 0.28, 0.27, 0.31, 0.3, 0.29, 0.25, 0.23, 0.3, 0.26, 0.26, 0.29, 0.28, 0.34, 0.23, 0.28, 0.26, 0.3, 0.32, 0.28, 0.22, 0.23, 0.24, 0.25, 0.27, 0.24, 0.29, 0.33, 0.32, 0.25, 0.26, 0.15, 0.28, 0.22, 0.28, 0.32, 0.26, 0.2, 0.25, 0.24, 0.22, 0.31, 0.26, 0.33, 0.21, 0.29, 0.24, 0.24, 0.3, 0.29, 0.39, 0.3, 0.23, 0.26, 0.26, 0.29, 0.24, 0.25, 0.24, 0.23, 0.25, 0.27, 0.26, 0.21, 0.28, 0.24, 0.28, 0.28, 0.28, 0.23, 0.25, 0.26, 0.23, 0.27, 0.23, 0.18, 0.25, 0.29, 0.28, 0.3, 0.25, 0.25, 0.24, 0.24, 0.26, 0.24, 0.28, 0.28, 0.26, 0.3, 0.33, 0.26, 0.26, 0.23, 0.32, 0.27, 0.29, 0.21, 0.23, 0.26, 0.25, 0.23, 0.25, 0.21, 0.27, 0.31, 0.24, 0.22, 0.3, 0.18, 0.28, 0.17, 0.24, 0.31, 0.27, 0.21, 0.35, 0.23, 0.3, 0.25, 0.23, 0.31, 0.3, 0.25, 0.25, 0.26, 0.26, 0.29, 0.3, 0.26, 0.24, 0.28, 0.29, 0.24, 0.27, 0.34, 0.23, 0.35, 0.27, 0.27, 0.3, 0.29, 0.31, 0.27, 0.26, 0.35, 0.3, 0.25, 0.21, 0.29, 0.21, 0.24, 0.24, 0.3, 0.34, 0.26, 0.25, 0.26, 0.22, 0.24, 0.3, 0.19, 0.28, 0.27, 0.23, 0.32, 0.29, 0.18, 0.3, 0.29, 0.3, 0.27, 0.28, 0.25, 0.23, 0.28, 0.27, 0.25, 0.31, 0.23, 0.29, 0.33, 0.3, 0.28, 0.29, 0.28, 0.32, 0.29, 0.24, 0.28, 0.27, 0.23, 0.21, 0.27, 0.34, 0.26, 0.27, 0.23, 0.25, 0.25, 0.3, 0.21, 0.15, 0.33, 0.29, 0.27, 0.27, 0.16, 0.3, 0.34, 0.23, 0.26, 0.19, 0.27, 0.3, 0.27, 0.26, 0.28, 0.18, 0.23, 0.27, 0.29, 0.28, 0.21, 0.29, 0.29, 0.24, 0.23, 0.27, 0.32, 0.22, 0.27, 0.29, 0.21, 0.28, 0.28, 0.33, 0.35, 0.41, 0.3, 0.25, 0.27, 0.31, 0.29, 0.29, 0.28, 0.29, 0.23, 0.28, 0.26, 0.23, 0.3, 0.22, 0.24, 0.26, 0.28, 0.23, 0.24, 0.3, 0.26, 0.22, 0.25, 0.33, 0.34, 0.21, 0.18, 0.24, 0.32, 0.23, 0.3, 0.21, 0.2, 0.27, 0.22, 0.33, 0.25, 0.27, 0.33, 0.23, 0.22, 0.19, 0.28, 0.28, 0.24, 0.28, 0.31, 0.26, 0.29, 0.24, 0.23, 0.26, 0.32, 0.35, 0.31, 0.26, 0.3, 0.31, 0.21, 0.27, 0.2, 0.27, 0.24, 0.29, 0.24, 0.34, 0.31, 0.21, 0.28, 0.27, 0.35, 0.24, 0.28, 0.26, 0.26, 0.22, 0.23, 0.27, 0.24, 0.23, 0.22, 0.23, 0.33, 0.26, 0.3, 0.28, 0.23, 0.27, 0.2, 0.24, 0.25, 0.28, 0.29, 0.19, 0.28, 0.22, 0.24, 0.27, 0.27, 0.3, 0.33, 0.29, 0.31, 0.25, 0.25, 0.28, 0.21, 0.27, 0.25, 0.21, 0.3, 0.26, 0.29, 0.27, 0.29, 0.3, 0.25, 0.25, 0.2, 0.31, 0.17, 0.2, 0.32, 0.21, 0.24, 0.24, 0.32, 0.21, 0.22, 0.3, 0.3, 0.23, 0.2, 0.22, 0.26, 0.28, 0.28, 0.23, 0.25, 0.25, 0.28, 0.24, 0.27, 0.26, 0.27, 0.24, 0.27, 0.25, 0.24, 0.34, 0.31, 0.33, 0.28, 0.19, 0.28, 0.29, 0.23, 0.3, 0.3, 0.2, 0.23, 0.3, 0.27, 0.28, 0.28, 0.25, 0.31, 0.31, 0.26, 0.3, 0.24, 0.31, 0.3, 0.27, 0.26, 0.27, 0.26, 0.28, 0.39, 0.28, 0.29, 0.26, 0.29, 0.23, 0.23, 0.25, 0.25, 0.33, 0.33, 0.25, 0.3, 0.27, 0.23, 0.3, 0.24, 0.27, 0.24, 0.25, 0.31, 0.26, 0.29, 0.22, 0.25, 0.24, 0.29, 0.15, 0.23, 0.31, 0.25, 0.23, 0.25, 0.29, 0.21, 0.23, 0.27, 0.29, 0.25, 0.28, 0.21, 0.26, 0.23, 0.29, 0.25, 0.26, 0.28, 0.26, 0.25, 0.28, 0.2, 0.32, 0.2, 0.22, 0.25, 0.29, 0.31, 0.23, 0.28, 0.24, 0.2, 0.26, 0.38, 0.22, 0.28, 0.28, 0.29, 0.27, 0.27, 0.21, 0.26, 0.27, 0.24, 0.25, 0.34, 0.22, 0.29, 0.24, 0.24, 0.32, 0.3, 0.29, 0.27, 0.26, 0.27, 0.23, 0.28, 0.3, 0.23, 0.24, 0.33, 0.23, 0.27, 0.3, 0.24, 0.31, 0.34, 0.26, 0.35, 0.21, 0.22, 0.25, 0.18, 0.34, 0.33, 0.23, 0.17, 0.27, 0.24, 0.25, 0.25, 0.25, 0.25, 0.28, 0.3, 0.24, 0.22, 0.23, 0.3, 0.25, 0.25, 0.27, 0.29, 0.26, 0.3, 0.19, 0.25, 0.25, 0.28, 0.22, 0.3, 0.29, 0.29, 0.2, 0.3, 0.26, 0.26, 0.28, 0.23, 0.24, 0.27, 0.23, 0.23, 0.22, 0.29, 0.2, 0.29, 0.35, 0.28, 0.32, 0.3, 0.31, 0.27, 0.29, 0.29, 0.36, 0.32, 0.21, 0.25, 0.2, 0.32, 0.27, 0.3, 0.23, 0.3, 0.27, 0.31, 0.29, 0.23, 0.22, 0.27, 0.24, 0.28, 0.29, 0.26, 0.3, 0.27, 0.38, 0.23, 0.2, 0.28, 0.26, 0.27, 0.28, 0.22, 0.23, 0.25, 0.26, 0.22, 0.27, 0.22, 0.2, 0.32, 0.25, 0.27, 0.27, 0.26, 0.24, 0.23, 0.25, 0.3, 0.28, 0.24, 0.25, 0.23, 0.23, 0.36, 0.29, 0.23, 0.24, 0.27, 0.27, 0.24, 0.28, 0.2, 0.32, 0.27, 0.27, 0.23, 0.27, 0.35, 0.21, 0.26, 0.31, 0.2, 0.34, 0.27, 0.29, 0.27, 0.31, 0.24, 0.3, 0.3, 0.36, 0.3, 0.22, 0.19, 0.32, 0.33, 0.25, 0.32, 0.24, 0.25, 0.3, 0.25, 0.27, 0.26, 0.32, 0.22, 0.24, 0.24, 0.25, 0.22, 0.25, 0.24, 0.28, 0.26, 0.25, 0.26, 0.24, 0.33, 0.29, 0.29, 0.29, 0.28, 0.23, 0.27, 0.28, 0.25, 0.18, 0.3, 0.26, 0.26, 0.27, 0.24, 0.33, 0.31, 0.31, 0.33, 0.25, 0.26, 0.27, 0.23, 0.22, 0.3, 0.27, 0.31, 0.26, 0.2, 0.26, 0.26, 0.27, 0.24, 0.28, 0.39, 0.28, 0.31, 0.28, 0.26, 0.27, 0.26, 0.28, 0.29, 0.24, 0.28, 0.25, 0.17, 0.23, 0.34, 0.27, 0.35, 0.28, 0.28, 0.24, 0.27, 0.22, 0.22, 0.33, 0.3, 0.26, 0.23, 0.26, 0.27, 0.31, 0.26, 0.27, 0.28, 0.28, 0.26, 0.39, 0.31, 0.33, 0.28, 0.27, 0.29, 0.34, 0.24, 0.27, 0.3, 0.26, 0.26, 0.28, 0.27, 0.32, 0.23, 0.23, 0.27, 0.23, 0.19, 0.29, 0.25, 0.24, 0.26, 0.33, 0.24, 0.24, 0.33, 0.3, 0.25, 0.26, 0.32, 0.27, 0.25, 0.23, 0.31, 0.3, 0.29, 0.27, 0.28, 0.26, 0.35, 0.26, 0.22, 0.29, 0.33, 0.26, 0.36, 0.31, 0.25, 0.31, 0.29, 0.32, 0.25, 0.26, 0.25, 0.29, 0.26, 0.28, 0.27, 0.3, 0.25, 0.22, 0.28, 0.29, 0.29, 0.18, 0.27, 0.23, 0.22, 0.23, 0.29, 0.33, 0.31, 0.29, 0.29, 0.27, 0.22, 0.21, 0.18, 0.3, 0.26, 0.24, 0.26, 0.24, 0.3, 0.25, 0.27, 0.28, 0.25, 0.31, 0.28, 0.26, 0.22, 0.24, 0.26, 0.24, 0.28, 0.22, 0.34, 0.31, 0.28, 0.26, 0.25, 0.22, 0.21, 0.37, 0.28, 0.27, 0.22, 0.28, 0.2, 0.3, 0.24, 0.29, 0.24, 0.24, 0.24, 0.25, 0.3, 0.21, 0.28, 0.26, 0.28, 0.24, 0.26, 0.24, 0.3, 0.29, 0.27, 0.3, 0.25, 0.28, 0.24, 0.27, 0.17, 0.25, 0.24, 0.28, 0.33, 0.26, 0.3, 0.32, 0.23, 0.35, 0.34, 0.3, 0.29, 0.21, 0.22, 0.3, 0.21, 0.25, 0.17, 0.33, 0.22, 0.2, 0.29, 0.28, 0.22, 0.25, 0.32, 0.22, 0.34, 0.25, 0.27, 0.25, 0.31, 0.28, 0.28, 0.15, 0.21, 0.19, 0.2, 0.34, 0.24, 0.22, 0.26, 0.2, 0.24, 0.29, 0.3, 0.22, 0.28, 0.24, 0.27, 0.29, 0.28, 0.24, 0.22, 0.28, 0.24, 0.27, 0.31, 0.33, 0.3, 0.28, 0.25, 0.28, 0.29, 0.17, 0.25, 0.24, 0.25, 0.24, 0.28, 0.25, 0.25, 0.28, 0.28, 0.21, 0.26, 0.29, 0.24, 0.26, 0.24, 0.2, 0.26, 0.25, 0.23, 0.3, 0.24, 0.34, 0.26, 0.27, 0.27, 0.24, 0.27, 0.31, 0.28, 0.26, 0.27, 0.23, 0.29, 0.33, 0.27, 0.23, 0.3, 0.23, 0.31, 0.29, 0.27, 0.19, 0.34, 0.2, 0.26, 0.27, 0.29, 0.28, 0.23, 0.3, 0.3, 0.26, 0.21, 0.3, 0.27, 0.27, 0.27, 0.26, 0.27, 0.22, 0.31, 0.32, 0.32, 0.3, 0.31, 0.27, 0.3, 0.22, 0.3, 0.13, 0.29, 0.24, 0.26, 0.19, 0.22, 0.29, 0.31, 0.26, 0.32, 0.28, 0.27, 0.3, 0.3, 0.23, 0.27, 0.15, 0.27, 0.3, 0.35, 0.26, 0.28, 0.26, 0.25, 0.27, 0.2, 0.26, 0.17, 0.22, 0.2, 0.25, 0.28, 0.24, 0.3, 0.21, 0.36, 0.34, 0.22, 0.26, 0.24, 0.28, 0.26, 0.22, 0.35, 0.2, 0.28, 0.15, 0.3, 0.25, 0.42, 0.24, 0.35, 0.26, 0.32, 0.3, 0.31, 0.24, 0.27, 0.22, 0.21, 0.26, 0.2, 0.23, 0.22, 0.26, 0.3, 0.23, 0.25, 0.27, 0.3, 0.32, 0.18, 0.33, 0.28, 0.28, 0.18, 0.29, 0.22, 0.27, 0.24, 0.25, 0.3, 0.33, 0.34, 0.25, 0.31, 0.2, 0.24, 0.25, 0.23, 0.28, 0.29, 0.23, 0.26, 0.26, 0.26, 0.26, 0.31, 0.25, 0.31, 0.3, 0.24, 0.22, 0.24, 0.23, 0.24, 0.26, 0.21, 0.27, 0.19, 0.25, 0.26, 0.35, 0.26, 0.22, 0.24, 0.22, 0.29, 0.3, 0.19, 0.27, 0.28, 0.32, 0.23, 0.18, 0.25, 0.3, 0.26, 0.26, 0.27, 0.35, 0.26, 0.23, 0.22, 0.35, 0.22, 0.33, 0.25, 0.26, 0.33, 0.26, 0.28, 0.19, 0.29, 0.27, 0.22, 0.22, 0.29, 0.25, 0.24, 0.31, 0.27, 0.25, 0.27, 0.29, 0.3, 0.34, 0.3, 0.25, 0.36, 0.27, 0.26, 0.27, 0.26, 0.2, 0.31, 0.27, 0.32, 0.25, 0.25, 0.26, 0.25, 0.31, 0.26, 0.21, 0.4, 0.23, 0.25, 0.33, 0.19, 0.24, 0.32, 0.29, 0.29, 0.25, 0.28, 0.34, 0.23, 0.24, 0.27, 0.19, 0.27, 0.27, 0.29, 0.3, 0.31, 0.21, 0.18, 0.29, 0.3, 0.24, 0.2, 0.34, 0.31, 0.29, 0.26, 0.24, 0.21, 0.23, 0.22, 0.3, 0.28, 0.28, 0.25, 0.24, 0.22, 0.3, 0.31, 0.27, 0.33, 0.23, 0.28, 0.24, 0.31, 0.33, 0.23, 0.22, 0.24, 0.28, 0.27, 0.32, 0.22, 0.21, 0.2, 0.22, 0.26, 0.33, 0.27, 0.25, 0.28, 0.24, 0.23, 0.2, 0.31, 0.25, 0.24, 0.27, 0.29, 0.26, 0.3, 0.39, 0.29, 0.25, 0.29, 0.32, 0.35, 0.24, 0.32, 0.3, 0.21, 0.24, 0.27, 0.22, 0.3, 0.28, 0.23, 0.3, 0.28, 0.23, 0.27, 0.29, 0.24, 0.24, 0.27, 0.22, 0.3, 0.33, 0.28, 0.32, 0.26, 0.25, 0.29, 0.28, 0.26, 0.26, 0.28, 0.33, 0.25, 0.3, 0.28, 0.31, 0.28, 0.23, 0.28, 0.22, 0.22, 0.28, 0.2, 0.25, 0.23, 0.29, 0.27, 0.24, 0.28, 0.24, 0.31, 0.28, 0.22, 0.34, 0.28, 0.32, 0.29, 0.2, 0.24, 0.23, 0.27, 0.25, 0.2, 0.24, 0.27, 0.28, 0.27, 0.23, 0.29, 0.26, 0.2, 0.27, 0.24, 0.28, 0.28, 0.29, 0.29, 0.25, 0.32, 0.22, 0.24, 0.25, 0.2, 0.25, 0.23, 0.24, 0.25, 0.23, 0.28, 0.29, 0.32, 0.27, 0.3, 0.26, 0.29, 0.24, 0.24, 0.31, 0.31, 0.28, 0.28, 0.3, 0.2, 0.3, 0.22, 0.27, 0.21, 0.29, 0.33, 0.3, 0.28, 0.28, 0.29, 0.24, 0.3, 0.22, 0.27, 0.28, 0.26, 0.25, 0.24, 0.23, 0.27, 0.34, 0.28, 0.28, 0.24, 0.29, 0.19, 0.25, 0.26, 0.22, 0.26, 0.22, 0.28, 0.31, 0.31, 0.22, 0.29, 0.25, 0.25, 0.28, 0.29, 0.23, 0.31, 0.27, 0.25, 0.26, 0.25, 0.25, 0.35, 0.3, 0.29, 0.3, 0.34, 0.31, 0.23, 0.24, 0.19, 0.27, 0.26, 0.26, 0.35, 0.24, 0.23, 0.24, 0.25, 0.28, 0.25, 0.22, 0.35, 0.28, 0.32, 0.24, 0.27, 0.25, 0.26, 0.26, 0.3, 0.26, 0.21, 0.24, 0.19, 0.23, 0.29, 0.28, 0.24, 0.21, 0.28, 0.32, 0.29, 0.28, 0.23, 0.27, 0.3, 0.22, 0.25, 0.28, 0.28, 0.23, 0.22, 0.23, 0.24, 0.23, 0.3, 0.23, 0.22, 0.29, 0.29, 0.27, 0.27, 0.24, 0.25, 0.23, 0.26, 0.27, 0.31, 0.28, 0.32, 0.26, 0.18, 0.26, 0.28, 0.22, 0.25, 0.21, 0.28, 0.22, 0.28, 0.24, 0.32, 0.23, 0.28, 0.3, 0.27, 0.2, 0.21, 0.26, 0.3, 0.2, 0.29, 0.28, 0.22, 0.22, 0.24, 0.33, 0.27, 0.29, 0.27, 0.21, 0.18, 0.32, 0.24, 0.21, 0.25, 0.25, 0.22, 0.29, 0.32, 0.21, 0.32, 0.24, 0.27, 0.24, 0.24, 0.23, 0.24, 0.21, 0.23, 0.36, 0.23, 0.31, 0.28, 0.27, 0.21, 0.27, 0.23, 0.25, 0.24, 0.21, 0.26, 0.3, 0.28, 0.28, 0.29, 0.24, 0.27, 0.25, 0.27, 0.27, 0.35, 0.23, 0.21, 0.15, 0.32, 0.29, 0.23, 0.22, 0.29, 0.2, 0.27, 0.23, 0.29, 0.29, 0.23, 0.25, 0.33, 0.27, 0.28, 0.24, 0.31, 0.28, 0.33, 0.26, 0.3, 0.29, 0.29, 0.26, 0.24, 0.24, 0.21, 0.26, 0.27, 0.28, 0.24, 0.38, 0.21, 0.32, 0.3, 0.28, 0.22, 0.34, 0.24, 0.24, 0.32, 0.26, 0.33, 0.25, 0.21, 0.28, 0.25, 0.27, 0.3, 0.25, 0.28, 0.26, 0.27, 0.25, 0.24, 0.29, 0.31, 0.24, 0.32, 0.25, 0.23, 0.35, 0.29, 0.32, 0.27, 0.34, 0.3, 0.23, 0.3, 0.24, 0.27, 0.21, 0.19, 0.32, 0.36, 0.3, 0.31, 0.33, 0.26, 0.28, 0.32, 0.27, 0.21, 0.28, 0.26, 0.22, 0.33, 0.33, 0.23, 0.25, 0.28, 0.28, 0.31, 0.25, 0.31, 0.33, 0.23, 0.31, 0.24, 0.25, 0.22, 0.3, 0.31, 0.16, 0.25, 0.28, 0.2, 0.21, 0.23, 0.26, 0.24, 0.3, 0.2, 0.22, 0.29, 0.24, 0.28, 0.3, 0.29, 0.33, 0.35, 0.22, 0.22, 0.24, 0.28, 0.26, 0.2, 0.35, 0.2, 0.26, 0.23, 0.29, 0.26, 0.25, 0.22, 0.25, 0.24, 0.22, 0.28, 0.34, 0.31, 0.23, 0.35, 0.25, 0.18, 0.27, 0.29, 0.21, 0.18, 0.22, 0.33, 0.27, 0.22, 0.31, 0.22, 0.26, 0.26, 0.24, 0.24, 0.27, 0.25, 0.28, 0.34, 0.24, 0.18, 0.23, 0.28, 0.3, 0.26, 0.25, 0.22, 0.22, 0.24, 0.28, 0.29, 0.16, 0.35, 0.31, 0.27, 0.25, 0.33, 0.25, 0.3, 0.35, 0.23, 0.27, 0.26, 0.26, 0.23, 0.3, 0.32, 0.2, 0.32, 0.25, 0.33, 0.28, 0.31, 0.27, 0.24, 0.29, 0.33, 0.37, 0.27, 0.27, 0.32, 0.25, 0.23, 0.25, 0.28, 0.29, 0.34, 0.19, 0.29, 0.31, 0.27, 0.27, 0.22, 0.19, 0.33, 0.26, 0.24, 0.3, 0.33, 0.25, 0.25, 0.27, 0.25, 0.21, 0.35, 0.29, 0.27, 0.25, 0.19, 0.23, 0.24, 0.28, 0.26, 0.22, 0.28, 0.21, 0.28, 0.18, 0.24, 0.29, 0.21, 0.28, 0.23, 0.28, 0.22, 0.33, 0.25, 0.28, 0.31, 0.24, 0.3, 0.28, 0.25, 0.25, 0.21, 0.27, 0.25, 0.24, 0.3, 0.26, 0.26, 0.28, 0.29, 0.28, 0.28, 0.19, 0.29, 0.29, 0.27, 0.25, 0.31, 0.26, 0.33, 0.31, 0.24, 0.26, 0.22, 0.26, 0.28, 0.26, 0.28, 0.26, 0.25, 0.25, 0.3, 0.32, 0.27, 0.3, 0.26, 0.31, 0.26, 0.26, 0.23, 0.23, 0.23, 0.24, 0.27, 0.24, 0.28, 0.25, 0.27, 0.22, 0.23, 0.29, 0.26, 0.28, 0.3, 0.28, 0.26, 0.27, 0.27, 0.32, 0.28, 0.21, 0.29, 0.29, 0.3, 0.23, 0.27, 0.31, 0.28, 0.24, 0.27, 0.18, 0.27, 0.31, 0.23, 0.26, 0.28, 0.28, 0.27, 0.24, 0.22, 0.34, 0.24, 0.27, 0.26, 0.26, 0.32, 0.26, 0.23, 0.25, 0.27, 0.23, 0.29, 0.23, 0.27, 0.25, 0.33, 0.24, 0.23, 0.24, 0.24, 0.23, 0.28, 0.22, 0.26, 0.22, 0.3, 0.23, 0.3, 0.25, 0.22, 0.28, 0.32, 0.24, 0.33, 0.26, 0.25, 0.29, 0.31, 0.25, 0.29, 0.24, 0.28, 0.19, 0.26, 0.32, 0.23, 0.24, 0.23, 0.28, 0.26, 0.21, 0.32, 0.25, 0.31, 0.31, 0.28, 0.3, 0.27, 0.31, 0.23, 0.26, 0.27, 0.18, 0.3, 0.29, 0.26, 0.21, 0.31, 0.26, 0.28, 0.23, 0.3, 0.26, 0.23, 0.32, 0.35, 0.25, 0.27, 0.27, 0.29, 0.24, 0.23, 0.23, 0.3, 0.25, 0.27, 0.23, 0.21, 0.24, 0.3, 0.24, 0.3, 0.29, 0.32, 0.21, 0.29, 0.28, 0.34, 0.22, 0.25, 0.26, 0.33, 0.29, 0.29, 0.23, 0.26, 0.27, 0.25, 0.21, 0.34, 0.24, 0.27, 0.18, 0.3, 0.29, 0.25, 0.28, 0.26, 0.32, 0.26, 0.34, 0.27, 0.19, 0.33, 0.18, 0.29, 0.22, 0.37, 0.26, 0.25, 0.3, 0.24, 0.25, 0.29, 0.25, 0.25, 0.26, 0.3, 0.23, 0.31, 0.32, 0.3, 0.22, 0.27, 0.3, 0.32, 0.28, 0.22, 0.28, 0.26, 0.35, 0.28, 0.27, 0.25, 0.21, 0.28, 0.28, 0.29, 0.21, 0.27, 0.24, 0.27, 0.24, 0.32, 0.29, 0.25, 0.29, 0.22, 0.24, 0.24, 0.27, 0.24, 0.29, 0.21, 0.34, 0.33, 0.33, 0.21, 0.3, 0.22, 0.26, 0.3, 0.18, 0.3, 0.28, 0.23, 0.27, 0.25, 0.32, 0.3, 0.22, 0.29, 0.24, 0.25, 0.28, 0.24, 0.28, 0.28, 0.19, 0.22, 0.26, 0.24, 0.3, 0.3, 0.31, 0.3, 0.3, 0.24, 0.25, 0.32, 0.23, 0.27, 0.32, 0.3, 0.23, 0.23, 0.29, 0.29, 0.26, 0.28, 0.21, 0.33, 0.25, 0.29, 0.27, 0.25, 0.29, 0.22, 0.27, 0.21, 0.24, 0.3, 0.25, 0.25, 0.26, 0.24, 0.3, 0.26, 0.28, 0.29, 0.25, 0.28, 0.31, 0.26, 0.22, 0.27, 0.23, 0.23, 0.19, 0.29, 0.25, 0.19, 0.33, 0.26, 0.26, 0.19, 0.21, 0.21, 0.23, 0.3, 0.27, 0.28, 0.32, 0.23, 0.25, 0.26, 0.27, 0.33, 0.34, 0.21, 0.29, 0.29, 0.24, 0.27, 0.32, 0.28, 0.26, 0.27, 0.19, 0.34, 0.29, 0.22, 0.24, 0.3, 0.23, 0.23, 0.18, 0.23, 0.22, 0.29, 0.3, 0.22, 0.31, 0.27, 0.25, 0.18, 0.23, 0.2, 0.26, 0.22, 0.29, 0.25, 0.21, 0.31, 0.32, 0.31, 0.23, 0.23, 0.26, 0.33, 0.31, 0.27, 0.32, 0.2, 0.29, 0.21, 0.32, 0.24, 0.21, 0.28, 0.17, 0.31, 0.3, 0.29, 0.24, 0.31, 0.31, 0.26, 0.25, 0.3, 0.22, 0.26, 0.28, 0.24, 0.26, 0.32, 0.29, 0.25, 0.31, 0.28, 0.21, 0.28, 0.3, 0.23, 0.33, 0.3, 0.25, 0.26, 0.25, 0.34, 0.23, 0.25, 0.22, 0.27, 0.26, 0.36, 0.2, 0.28, 0.25, 0.26, 0.23, 0.25, 0.22, 0.19, 0.23, 0.31, 0.34, 0.16, 0.29, 0.27, 0.26, 0.28, 0.27, 0.27, 0.24, 0.22, 0.17, 0.25, 0.24, 0.23, 0.27, 0.17, 0.29, 0.29, 0.24, 0.26, 0.27, 0.24, 0.28, 0.26, 0.21, 0.31, 0.29, 0.24, 0.28, 0.27, 0.2, 0.27, 0.29, 0.31, 0.31, 0.28, 0.3, 0.18, 0.21, 0.27, 0.28, 0.32, 0.25, 0.26, 0.21, 0.25, 0.22, 0.26, 0.31, 0.31, 0.29, 0.26, 0.36, 0.27, 0.27, 0.21, 0.26, 0.22, 0.31, 0.24, 0.33, 0.26, 0.26, 0.26, 0.25, 0.18, 0.31, 0.21, 0.26, 0.2, 0.26, 0.23, 0.22, 0.29, 0.27, 0.23, 0.26, 0.22, 0.28, 0.25, 0.25, 0.25, 0.27, 0.34, 0.3, 0.27, 0.26, 0.31, 0.26, 0.19, 0.3, 0.24, 0.23, 0.2, 0.23, 0.3, 0.29, 0.28, 0.29, 0.22, 0.22, 0.23, 0.31, 0.26, 0.26, 0.29, 0.25, 0.26, 0.2, 0.25, 0.22, 0.25, 0.23, 0.27, 0.28, 0.23, 0.29, 0.33, 0.3, 0.28, 0.25, 0.24, 0.28, 0.22, 0.33, 0.22, 0.25, 0.27, 0.27, 0.37, 0.2, 0.26, 0.23, 0.28, 0.23, 0.27, 0.25, 0.3, 0.3, 0.25, 0.24, 0.32, 0.27, 0.2, 0.22, 0.25, 0.31, 0.28, 0.27, 0.3, 0.21, 0.26, 0.25, 0.28, 0.27, 0.24, 0.3, 0.21, 0.3, 0.24, 0.22, 0.3, 0.32, 0.25, 0.26, 0.34, 0.31, 0.2, 0.28, 0.3, 0.27, 0.29, 0.25, 0.23, 0.31, 0.28, 0.31, 0.29, 0.25, 0.28, 0.32, 0.19, 0.29, 0.3, 0.21, 0.31, 0.28, 0.27, 0.31, 0.29, 0.21, 0.32, 0.22, 0.31, 0.26, 0.21, 0.23, 0.26, 0.24, 0.28, 0.29, 0.26, 0.26, 0.27, 0.23, 0.36, 0.3, 0.29, 0.28, 0.2, 0.28, 0.28, 0.29, 0.21, 0.27, 0.29, 0.32, 0.24, 0.26, 0.3, 0.24, 0.25, 0.28, 0.29, 0.31, 0.27, 0.24, 0.28, 0.23, 0.26, 0.26, 0.29, 0.24, 0.24, 0.31, 0.24, 0.21, 0.26, 0.26, 0.23, 0.32, 0.25, 0.25, 0.24, 0.26, 0.24, 0.26, 0.29, 0.23, 0.24, 0.28, 0.28, 0.25, 0.31, 0.24, 0.37, 0.28, 0.19, 0.24, 0.26, 0.25, 0.27, 0.28, 0.23, 0.25, 0.21, 0.23, 0.28, 0.35, 0.31, 0.32, 0.27, 0.3, 0.26, 0.27, 0.3, 0.27, 0.24, 0.31, 0.31, 0.23, 0.25, 0.25, 0.21, 0.31, 0.25, 0.27, 0.23, 0.29, 0.28, 0.3, 0.22, 0.27, 0.23, 0.28, 0.27, 0.29, 0.31, 0.28, 0.32, 0.34, 0.32, 0.24, 0.26, 0.27, 0.26, 0.29, 0.22, 0.23, 0.3, 0.32, 0.24, 0.25, 0.26, 0.2, 0.3, 0.32, 0.23, 0.27, 0.32, 0.31, 0.33, 0.29, 0.38, 0.35, 0.29, 0.27, 0.2, 0.23, 0.23, 0.24, 0.24, 0.19, 0.25, 0.26, 0.29, 0.26, 0.24, 0.24, 0.24, 0.31, 0.27, 0.28, 0.21, 0.23, 0.22, 0.25, 0.21, 0.25, 0.34, 0.25, 0.25, 0.32, 0.26, 0.23, 0.23, 0.24, 0.32, 0.32, 0.22, 0.24, 0.27, 0.33, 0.25, 0.32, 0.34, 0.31, 0.3, 0.27, 0.27, 0.24, 0.25, 0.28, 0.23, 0.21, 0.29, 0.28, 0.3, 0.23, 0.25, 0.24, 0.26, 0.25, 0.19, 0.29, 0.28, 0.3, 0.21, 0.26, 0.3, 0.24, 0.27, 0.24, 0.27, 0.24, 0.35, 0.24, 0.23, 0.21, 0.21, 0.31, 0.29, 0.26, 0.27, 0.27, 0.32, 0.3, 0.27, 0.24, 0.32, 0.21, 0.23, 0.32, 0.3, 0.31, 0.29, 0.29, 0.24, 0.31, 0.31, 0.25, 0.29, 0.24, 0.33, 0.27, 0.25, 0.3, 0.24, 0.24, 0.28, 0.28, 0.22, 0.31, 0.31, 0.29, 0.22, 0.26, 0.26, 0.22, 0.29, 0.31, 0.27, 0.26, 0.27, 0.28, 0.31, 0.27, 0.3, 0.23, 0.23, 0.21, 0.25, 0.27, 0.27, 0.27, 0.29, 0.28, 0.27, 0.24, 0.26, 0.25, 0.32, 0.27, 0.29, 0.25, 0.25, 0.26, 0.23, 0.28, 0.32, 0.32, 0.28, 0.31, 0.23, 0.27, 0.16, 0.22, 0.24, 0.28, 0.26, 0.24, 0.28, 0.31, 0.23, 0.28, 0.25, 0.3, 0.28, 0.18, 0.25, 0.26, 0.29, 0.34, 0.28, 0.23, 0.25, 0.23, 0.25, 0.23, 0.27, 0.31, 0.28, 0.23, 0.27, 0.18, 0.27, 0.29, 0.32, 0.26, 0.24, 0.29, 0.23, 0.24, 0.23, 0.29, 0.3, 0.23, 0.22, 0.27, 0.26, 0.28, 0.18, 0.26, 0.27, 0.29, 0.31, 0.28, 0.31, 0.26, 0.27, 0.27, 0.31, 0.24, 0.28, 0.29, 0.21, 0.28, 0.26, 0.22, 0.16, 0.2, 0.26, 0.22, 0.21, 0.19, 0.28, 0.26, 0.21, 0.27, 0.23, 0.27, 0.18, 0.21, 0.26, 0.32, 0.25, 0.28, 0.27, 0.31, 0.29, 0.21, 0.23, 0.26, 0.26, 0.27, 0.26, 0.26, 0.26, 0.2, 0.24, 0.27, 0.28, 0.29, 0.34, 0.27, 0.21, 0.28, 0.25, 0.32, 0.23, 0.21, 0.26, 0.31, 0.35, 0.27, 0.24, 0.22, 0.31, 0.3, 0.26, 0.33, 0.31, 0.3, 0.31, 0.22, 0.21, 0.23, 0.34, 0.26, 0.25, 0.2, 0.27, 0.28, 0.26, 0.23, 0.27, 0.24, 0.16, 0.24, 0.23, 0.24, 0.27, 0.26, 0.2, 0.26, 0.27, 0.28, 0.27, 0.19, 0.26, 0.24, 0.22, 0.31, 0.31, 0.3, 0.35, 0.21, 0.25, 0.23, 0.22, 0.22, 0.3, 0.27, 0.33, 0.33, 0.32, 0.26, 0.27, 0.35, 0.22, 0.34, 0.3, 0.34, 0.25, 0.22, 0.24, 0.26, 0.24, 0.23, 0.28, 0.21, 0.26, 0.23, 0.26, 0.25, 0.2, 0.3, 0.25, 0.21, 0.29, 0.27, 0.25, 0.28, 0.25, 0.31, 0.27, 0.25, 0.23, 0.21, 0.26, 0.28, 0.24, 0.31, 0.34, 0.26, 0.3, 0.32, 0.23, 0.23, 0.32, 0.24, 0.31, 0.39, 0.24, 0.26, 0.25, 0.24, 0.23, 0.25, 0.18, 0.22, 0.24, 0.32, 0.26, 0.31, 0.24, 0.34, 0.25, 0.25, 0.31, 0.31, 0.23, 0.26, 0.21, 0.32, 0.26, 0.29, 0.23, 0.24, 0.28, 0.22, 0.28, 0.25, 0.27, 0.24, 0.27, 0.27, 0.26, 0.28, 0.35, 0.23, 0.22, 0.23, 0.29, 0.26, 0.24, 0.26, 0.33, 0.21, 0.28, 0.18, 0.29, 0.3, 0.28, 0.28, 0.28, 0.28, 0.24, 0.27, 0.26, 0.26, 0.23, 0.24, 0.24, 0.28, 0.23, 0.23, 0.26, 0.34, 0.27, 0.25, 0.28, 0.26, 0.21, 0.24, 0.19, 0.24, 0.33, 0.26, 0.21, 0.3, 0.28, 0.26, 0.23, 0.31, 0.21, 0.27, 0.31, 0.32, 0.2, 0.32, 0.23, 0.27, 0.22, 0.25, 0.27, 0.24, 0.24, 0.27, 0.31, 0.3, 0.2, 0.24, 0.2, 0.2, 0.27, 0.29, 0.36, 0.23, 0.27, 0.31, 0.22, 0.27, 0.18, 0.29, 0.22, 0.3, 0.37, 0.29, 0.29, 0.26, 0.24, 0.25, 0.19, 0.23, 0.27, 0.24, 0.29, 0.34, 0.27, 0.31, 0.2, 0.22, 0.27, 0.24, 0.28, 0.23, 0.27, 0.3, 0.31, 0.28, 0.23, 0.29, 0.3, 0.24, 0.2, 0.25, 0.28, 0.22, 0.26, 0.28, 0.19, 0.19, 0.29, 0.34, 0.25, 0.19, 0.25, 0.19, 0.28, 0.3, 0.26, 0.24, 0.29, 0.28, 0.3, 0.31, 0.33, 0.18, 0.25, 0.26, 0.21, 0.25, 0.26, 0.29, 0.28, 0.24, 0.32, 0.23, 0.27, 0.31, 0.26, 0.25, 0.3, 0.34, 0.25, 0.25, 0.3, 0.29, 0.22, 0.24, 0.28, 0.29, 0.2, 0.25, 0.24, 0.3, 0.27, 0.29, 0.26, 0.27, 0.33, 0.23, 0.3, 0.3, 0.37, 0.28, 0.35, 0.28, 0.22, 0.29, 0.3, 0.26, 0.23, 0.28, 0.17, 0.29, 0.31, 0.31, 0.23, 0.27, 0.2, 0.25, 0.27, 0.25, 0.23, 0.26, 0.27, 0.35, 0.28, 0.3, 0.34, 0.27, 0.21, 0.3, 0.26, 0.24, 0.23, 0.29, 0.29, 0.18, 0.23, 0.3, 0.22, 0.27, 0.3, 0.26, 0.28, 0.3, 0.18, 0.23, 0.24, 0.26, 0.27, 0.16, 0.18, 0.24, 0.27, 0.22, 0.31, 0.27, 0.37, 0.25, 0.29, 0.27, 0.24, 0.24, 0.31, 0.21, 0.32, 0.19, 0.2, 0.27, 0.32, 0.2, 0.31, 0.24, 0.21, 0.25, 0.27, 0.28, 0.34, 0.23, 0.29, 0.23, 0.3, 0.25, 0.25, 0.29, 0.27, 0.28, 0.26, 0.25, 0.24, 0.17, 0.21, 0.34, 0.29, 0.24, 0.26, 0.28, 0.29, 0.31, 0.26, 0.29, 0.3, 0.3, 0.27, 0.34, 0.33, 0.25, 0.3, 0.2, 0.31, 0.27, 0.27, 0.24, 0.22, 0.26, 0.26, 0.26, 0.21, 0.22, 0.28, 0.3, 0.23, 0.28, 0.27, 0.32, 0.29, 0.22, 0.26, 0.25, 0.31, 0.25, 0.27, 0.25, 0.26, 0.23, 0.28, 0.2, 0.27, 0.26, 0.28, 0.26, 0.31, 0.22, 0.22, 0.25, 0.21, 0.27, 0.3, 0.27, 0.24, 0.26, 0.22, 0.23, 0.19, 0.26, 0.26, 0.23, 0.22, 0.33, 0.28, 0.28, 0.22, 0.3, 0.3, 0.32, 0.29, 0.22, 0.27, 0.25, 0.22, 0.23, 0.23, 0.26, 0.21, 0.29, 0.27, 0.19, 0.31, 0.28, 0.23, 0.3, 0.32, 0.29, 0.19, 0.23, 0.28, 0.34, 0.25, 0.22, 0.23, 0.36, 0.27, 0.3, 0.25, 0.26, 0.31, 0.3, 0.28, 0.18, 0.22, 0.2, 0.22, 0.26, 0.3, 0.26, 0.27, 0.29, 0.22, 0.25, 0.26, 0.24, 0.28, 0.31, 0.33, 0.31, 0.23, 0.27, 0.25, 0.32, 0.3, 0.2, 0.28, 0.28, 0.27, 0.18, 0.28, 0.23, 0.22, 0.28, 0.26, 0.24, 0.25, 0.32, 0.2, 0.28, 0.27, 0.32, 0.21, 0.26, 0.29, 0.32, 0.26, 0.26, 0.25, 0.29, 0.23, 0.23, 0.27, 0.31, 0.27, 0.23, 0.29, 0.23, 0.25, 0.26, 0.26, 0.25, 0.25, 0.27, 0.28, 0.26, 0.25, 0.28, 0.24, 0.29, 0.23, 0.23, 0.27, 0.29, 0.33, 0.25, 0.25, 0.22, 0.31, 0.2, 0.36, 0.26, 0.33, 0.3, 0.3, 0.29, 0.32, 0.27, 0.28, 0.3, 0.27, 0.23, 0.2, 0.2, 0.25, 0.25, 0.18, 0.27, 0.28, 0.29, 0.25, 0.24, 0.21, 0.31, 0.29, 0.24, 0.28, 0.33, 0.3, 0.23, 0.28, 0.33, 0.22, 0.26, 0.28, 0.31, 0.25, 0.28, 0.31, 0.26, 0.25, 0.3, 0.3, 0.21, 0.26, 0.3, 0.26, 0.26, 0.26, 0.29, 0.25, 0.26, 0.3, 0.29, 0.25, 0.19, 0.3, 0.21, 0.22, 0.25, 0.25, 0.25, 0.29, 0.31, 0.28, 0.25, 0.24, 0.25, 0.29, 0.28, 0.3, 0.25, 0.27, 0.27, 0.3, 0.3, 0.32, 0.17, 0.31, 0.24, 0.31, 0.3, 0.25, 0.27, 0.22, 0.34, 0.18, 0.31, 0.33, 0.22, 0.25, 0.23, 0.21, 0.28, 0.27, 0.27, 0.24, 0.25, 0.19, 0.3, 0.2, 0.26, 0.31, 0.22, 0.3, 0.25, 0.27, 0.25, 0.28, 0.29, 0.27, 0.24, 0.3, 0.26, 0.31, 0.28, 0.2, 0.31, 0.28, 0.31, 0.27, 0.24, 0.29, 0.31, 0.3, 0.25, 0.23, 0.29, 0.29, 0.32, 0.29, 0.28, 0.32, 0.28, 0.28, 0.29, 0.33, 0.23, 0.21, 0.29, 0.26, 0.3, 0.2, 0.27, 0.27, 0.25, 0.31, 0.26, 0.26, 0.25, 0.31, 0.25, 0.26, 0.25, 0.22, 0.27, 0.22, 0.25, 0.27, 0.32, 0.22, 0.3, 0.34, 0.22, 0.25, 0.22, 0.25, 0.31, 0.28, 0.28, 0.21, 0.27, 0.34, 0.22, 0.27, 0.23, 0.3, 0.23, 0.21, 0.3, 0.26, 0.3, 0.27, 0.32, 0.3, 0.24, 0.31, 0.3, 0.29, 0.24, 0.3, 0.28, 0.26, 0.26, 0.18, 0.3, 0.32, 0.24, 0.29, 0.37, 0.22, 0.22, 0.29, 0.26, 0.28, 0.25, 0.22, 0.27, 0.29, 0.28, 0.28, 0.23, 0.26, 0.22, 0.28, 0.17, 0.29, 0.33, 0.29, 0.36, 0.26, 0.27, 0.24, 0.25, 0.27, 0.23, 0.27, 0.35, 0.26, 0.23, 0.28, 0.27, 0.26, 0.28, 0.27, 0.24, 0.28, 0.23, 0.28, 0.25, 0.29, 0.24, 0.28, 0.28, 0.27, 0.28, 0.27, 0.25, 0.3, 0.24, 0.28, 0.29, 0.25, 0.27, 0.28, 0.31, 0.25, 0.31, 0.26, 0.28, 0.25, 0.31, 0.25, 0.26, 0.23, 0.28, 0.3, 0.2, 0.25, 0.28, 0.29, 0.24, 0.22, 0.23, 0.23, 0.21, 0.29, 0.17, 0.26, 0.22, 0.28, 0.2, 0.25, 0.34, 0.28, 0.32, 0.28, 0.27, 0.26, 0.28, 0.22, 0.2, 0.28, 0.17, 0.31, 0.28, 0.29, 0.29, 0.28, 0.29, 0.27, 0.31, 0.28, 0.25, 0.21, 0.25, 0.25, 0.2, 0.23, 0.27, 0.26, 0.27, 0.26, 0.33, 0.27, 0.24, 0.29, 0.3, 0.36, 0.29, 0.2, 0.23, 0.24, 0.29, 0.26, 0.21, 0.27, 0.21, 0.23, 0.25, 0.22, 0.3, 0.28, 0.2, 0.28, 0.17, 0.33, 0.22, 0.25, 0.25, 0.27, 0.3, 0.29, 0.28, 0.3, 0.27, 0.29, 0.24, 0.28, 0.3, 0.28, 0.29, 0.22, 0.26, 0.27, 0.3, 0.28, 0.23, 0.25, 0.3, 0.29, 0.34, 0.29, 0.29, 0.28, 0.3, 0.23, 0.23, 0.18, 0.24, 0.18, 0.27, 0.28, 0.23, 0.33, 0.31, 0.27, 0.26, 0.23, 0.29, 0.2, 0.23, 0.3, 0.27, 0.27, 0.27, 0.23, 0.23, 0.21, 0.28, 0.3, 0.31, 0.24, 0.21, 0.24, 0.28, 0.26, 0.22, 0.24, 0.22, 0.3, 0.28, 0.24, 0.3, 0.25, 0.31, 0.32, 0.25, 0.22, 0.28, 0.19, 0.21, 0.26, 0.27, 0.23, 0.29, 0.22, 0.27, 0.29, 0.33, 0.17, 0.32, 0.26, 0.26, 0.29, 0.26, 0.25, 0.24, 0.24, 0.22, 0.37, 0.25, 0.25, 0.19, 0.24, 0.28, 0.31, 0.22, 0.28, 0.27, 0.23, 0.29, 0.28, 0.35, 0.28, 0.3, 0.2, 0.25, 0.24, 0.21, 0.23, 0.31, 0.24, 0.29, 0.31, 0.25, 0.27, 0.28, 0.23, 0.25, 0.23, 0.25, 0.23, 0.3, 0.33, 0.26, 0.23, 0.28, 0.26, 0.2, 0.31, 0.34, 0.3, 0.31, 0.28, 0.26, 0.29, 0.27, 0.24, 0.25, 0.24, 0.24, 0.26, 0.23, 0.26, 0.25, 0.19, 0.27, 0.32, 0.19, 0.31, 0.35, 0.26, 0.21, 0.3, 0.25, 0.21, 0.25, 0.37, 0.19, 0.32, 0.22, 0.25, 0.22, 0.33, 0.21, 0.23, 0.23, 0.26, 0.22, 0.25, 0.35, 0.3, 0.24, 0.24, 0.28, 0.23, 0.24, 0.24, 0.24, 0.3, 0.28, 0.28, 0.25, 0.21, 0.28, 0.34, 0.24, 0.22, 0.26, 0.18, 0.24, 0.28, 0.22, 0.38, 0.26, 0.22, 0.27, 0.24, 0.33, 0.31, 0.27, 0.25, 0.24, 0.25, 0.25, 0.22, 0.26, 0.27, 0.24, 0.28, 0.31, 0.33, 0.31, 0.31, 0.27, 0.24, 0.32, 0.3, 0.27, 0.29, 0.27, 0.34, 0.28, 0.37, 0.27, 0.3, 0.24, 0.28, 0.21, 0.28, 0.29, 0.26, 0.27, 0.23, 0.19, 0.25, 0.24, 0.27, 0.25, 0.29, 0.3, 0.29, 0.22, 0.3, 0.26, 0.26, 0.22, 0.3, 0.27, 0.2, 0.25, 0.2, 0.22, 0.25, 0.26, 0.26, 0.26, 0.28, 0.28, 0.23, 0.26, 0.22, 0.28, 0.35, 0.28, 0.29, 0.23, 0.28, 0.26, 0.3, 0.2, 0.22, 0.22, 0.27, 0.26, 0.26, 0.32, 0.22, 0.24, 0.3, 0.21, 0.25, 0.29, 0.28, 0.28, 0.24, 0.24, 0.27, 0.24, 0.28, 0.28, 0.27, 0.21, 0.22, 0.26, 0.28, 0.22, 0.22, 0.25, 0.26, 0.24, 0.27, 0.3, 0.3, 0.29, 0.23, 0.25, 0.28, 0.28, 0.28, 0.3, 0.26, 0.29, 0.25, 0.29, 0.26, 0.2, 0.28, 0.25, 0.24, 0.33, 0.27, 0.22, 0.26, 0.27, 0.3, 0.22, 0.27, 0.25, 0.23, 0.26, 0.36, 0.19, 0.28, 0.3, 0.26, 0.24, 0.24, 0.27, 0.26, 0.24, 0.26, 0.3, 0.26, 0.23, 0.26, 0.26, 0.21, 0.28, 0.26, 0.34, 0.24, 0.29, 0.27, 0.2, 0.27, 0.25, 0.28, 0.27, 0.23, 0.28, 0.25, 0.34, 0.24, 0.28, 0.32, 0.33, 0.23, 0.23, 0.28, 0.39, 0.2, 0.27, 0.23, 0.33, 0.28, 0.21, 0.37, 0.32, 0.28, 0.23, 0.21, 0.26, 0.29, 0.21, 0.24, 0.26, 0.21, 0.23, 0.27, 0.34, 0.26, 0.23, 0.23, 0.21, 0.28, 0.19, 0.24, 0.29, 0.27, 0.23, 0.31, 0.23, 0.3, 0.29, 0.25, 0.27, 0.26, 0.22, 0.28, 0.27, 0.3, 0.19, 0.3, 0.3, 0.25, 0.29, 0.3, 0.23, 0.22, 0.31, 0.25, 0.25, 0.26, 0.21, 0.29, 0.21, 0.29, 0.21, 0.3, 0.3, 0.26, 0.32, 0.24, 0.27, 0.31, 0.27, 0.26, 0.26, 0.22, 0.27, 0.25, 0.25, 0.23, 0.29, 0.18, 0.28, 0.24, 0.23, 0.3, 0.28, 0.29, 0.27, 0.26, 0.26, 0.3, 0.29, 0.29, 0.19, 0.21, 0.23, 0.28, 0.28, 0.23, 0.25, 0.27, 0.23, 0.23, 0.35, 0.24, 0.24, 0.3, 0.3, 0.2, 0.29, 0.37, 0.23, 0.29, 0.28, 0.18, 0.3, 0.29, 0.17, 0.32, 0.22, 0.24, 0.19, 0.22, 0.29, 0.27, 0.31, 0.31, 0.25, 0.26, 0.2, 0.29, 0.17, 0.24, 0.22, 0.23, 0.3, 0.3, 0.26, 0.27, 0.28, 0.3, 0.3, 0.27, 0.22, 0.24, 0.21, 0.34, 0.2, 0.28, 0.29, 0.22, 0.3, 0.28, 0.29, 0.27, 0.27, 0.26, 0.23, 0.24, 0.34, 0.27, 0.28, 0.29, 0.3, 0.26, 0.26, 0.28, 0.26, 0.31, 0.26, 0.24, 0.26, 0.27, 0.3, 0.22, 0.23, 0.25, 0.26, 0.21, 0.3, 0.23, 0.27, 0.21, 0.25, 0.29, 0.29, 0.31, 0.26, 0.29, 0.31, 0.21, 0.22, 0.26, 0.26, 0.25, 0.29, 0.22, 0.26, 0.34, 0.3, 0.26, 0.24, 0.22, 0.27, 0.18, 0.25, 0.24, 0.3, 0.26, 0.2, 0.26, 0.28, 0.31, 0.25, 0.3, 0.29, 0.25, 0.27, 0.21, 0.26, 0.25, 0.33, 0.19, 0.25, 0.25, 0.28, 0.32, 0.28, 0.2, 0.25, 0.32, 0.3, 0.32, 0.3, 0.26, 0.34, 0.24, 0.36, 0.23, 0.21, 0.28, 0.26, 0.29, 0.26, 0.26, 0.27, 0.25, 0.32, 0.24, 0.24, 0.2, 0.29, 0.26, 0.28, 0.23, 0.28, 0.3, 0.31, 0.23, 0.22, 0.26, 0.28, 0.19, 0.26, 0.29, 0.24, 0.33, 0.25, 0.24, 0.22, 0.26, 0.28, 0.27, 0.25, 0.34, 0.32, 0.24, 0.26, 0.24, 0.24, 0.29, 0.28, 0.42, 0.32, 0.25, 0.27, 0.28, 0.29, 0.28, 0.27, 0.31, 0.28, 0.2, 0.28, 0.27, 0.26, 0.22, 0.3, 0.25, 0.27, 0.28, 0.23, 0.24, 0.26, 0.24, 0.29, 0.29, 0.25, 0.2, 0.31, 0.29, 0.28, 0.33, 0.27, 0.23, 0.2, 0.34, 0.22, 0.27, 0.28, 0.29, 0.24, 0.2, 0.29, 0.25, 0.22, 0.27, 0.29, 0.2, 0.32, 0.28, 0.3, 0.23, 0.26, 0.28, 0.24, 0.25, 0.27, 0.19, 0.3, 0.27, 0.27, 0.23, 0.22, 0.22, 0.26, 0.27, 0.26, 0.32, 0.26, 0.24, 0.26, 0.29, 0.24, 0.26, 0.25, 0.27, 0.24, 0.32, 0.26, 0.27, 0.3, 0.27, 0.24, 0.32, 0.25, 0.16, 0.23, 0.21, 0.24, 0.24, 0.36, 0.31, 0.23, 0.27, 0.28, 0.25, 0.27, 0.31, 0.3, 0.24, 0.25, 0.29, 0.24, 0.32, 0.37, 0.23, 0.29, 0.26, 0.25, 0.25, 0.28, 0.31, 0.26, 0.25, 0.27, 0.3, 0.28, 0.26, 0.29, 0.27, 0.28, 0.3, 0.18, 0.29, 0.24, 0.35, 0.32, 0.3, 0.27, 0.2, 0.21, 0.27, 0.2, 0.26, 0.23, 0.25, 0.22, 0.31, 0.33, 0.3, 0.26, 0.27, 0.25, 0.25, 0.3, 0.24, 0.27, 0.27, 0.24, 0.25, 0.2, 0.29, 0.33, 0.25, 0.28, 0.3, 0.25, 0.25, 0.3, 0.29, 0.25, 0.23, 0.29, 0.25, 0.23, 0.27, 0.26, 0.29, 0.35, 0.2, 0.23, 0.28, 0.29, 0.26, 0.28, 0.31, 0.3, 0.25, 0.22, 0.29, 0.31, 0.22, 0.28, 0.32, 0.26, 0.29, 0.19, 0.32, 0.32, 0.21, 0.28, 0.28, 0.26, 0.25, 0.28, 0.25, 0.27, 0.23, 0.28, 0.23, 0.26, 0.28, 0.29, 0.33, 0.28, 0.28, 0.2, 0.28, 0.23, 0.27, 0.23, 0.2, 0.3, 0.25, 0.28, 0.18, 0.3, 0.31, 0.27, 0.25, 0.27, 0.27, 0.23, 0.23, 0.3, 0.27, 0.29, 0.27, 0.28, 0.27, 0.24, 0.22, 0.29, 0.26, 0.22, 0.24, 0.21, 0.26, 0.24, 0.24, 0.27, 0.26, 0.29, 0.29, 0.36, 0.26, 0.27, 0.24, 0.27, 0.26, 0.24, 0.24, 0.3, 0.19, 0.31, 0.24, 0.27, 0.33, 0.27, 0.25, 0.26, 0.27, 0.25, 0.26, 0.26, 0.34, 0.24, 0.32, 0.21, 0.2, 0.2, 0.33, 0.26, 0.28, 0.23, 0.28, 0.2, 0.21, 0.27, 0.2, 0.33, 0.29, 0.36, 0.23, 0.26, 0.33, 0.28, 0.23, 0.29, 0.28, 0.27, 0.22, 0.27, 0.26, 0.26, 0.33, 0.34, 0.28, 0.29, 0.26, 0.28, 0.25, 0.28, 0.26, 0.23, 0.24, 0.29, 0.25, 0.25, 0.27, 0.29, 0.28, 0.36, 0.26, 0.29, 0.26, 0.31, 0.25, 0.36, 0.29, 0.27, 0.22, 0.25, 0.28, 0.27, 0.28, 0.24, 0.25, 0.25, 0.21, 0.27, 0.24, 0.29, 0.27, 0.3, 0.21, 0.31, 0.28, 0.22, 0.32, 0.31, 0.25, 0.29, 0.26, 0.26, 0.3, 0.23, 0.24, 0.3, 0.21, 0.25, 0.27, 0.3, 0.25, 0.29, 0.27, 0.22, 0.21, 0.31, 0.23, 0.27, 0.24, 0.28, 0.3, 0.23, 0.3, 0.33, 0.23, 0.34, 0.26, 0.26, 0.22, 0.24, 0.24, 0.26, 0.35, 0.27, 0.29, 0.27, 0.31, 0.29, 0.25, 0.28, 0.23, 0.25, 0.34, 0.28, 0.21, 0.28, 0.28, 0.27, 0.27, 0.34, 0.22, 0.24, 0.27, 0.27, 0.27, 0.27, 0.28, 0.31, 0.28, 0.31, 0.27, 0.25, 0.22, 0.28, 0.26, 0.24, 0.24, 0.27, 0.27, 0.27, 0.22, 0.28, 0.23, 0.32, 0.24, 0.24, 0.27, 0.35, 0.23, 0.28, 0.21, 0.26, 0.3, 0.26, 0.22, 0.28, 0.26, 0.26, 0.27, 0.32, 0.31, 0.23, 0.26, 0.29, 0.24, 0.28, 0.27, 0.24, 0.26, 0.22, 0.17, 0.26, 0.22, 0.24, 0.27, 0.25, 0.22, 0.25, 0.27, 0.26, 0.23, 0.22, 0.2, 0.28, 0.21, 0.29, 0.28, 0.2, 0.27, 0.25, 0.32, 0.24, 0.22, 0.25, 0.28, 0.24, 0.17, 0.23, 0.24, 0.32, 0.3, 0.29, 0.25, 0.3, 0.27, 0.37, 0.24, 0.27, 0.26, 0.26, 0.28, 0.3, 0.26, 0.3, 0.29, 0.26, 0.26, 0.29, 0.27, 0.22, 0.25, 0.22, 0.26, 0.31, 0.33, 0.29, 0.24, 0.23, 0.26, 0.3, 0.24, 0.25, 0.17, 0.23, 0.25, 0.28, 0.21, 0.25, 0.21, 0.29, 0.36, 0.31, 0.26, 0.19, 0.26, 0.31, 0.24, 0.27, 0.29, 0.21, 0.26, 0.28, 0.22, 0.23, 0.3, 0.23, 0.25, 0.28, 0.23, 0.26, 0.27, 0.26, 0.3, 0.27, 0.31, 0.23, 0.27, 0.29, 0.35, 0.32, 0.32, 0.27, 0.23, 0.24, 0.25, 0.26, 0.24, 0.28, 0.22, 0.36, 0.3, 0.38, 0.25, 0.25, 0.28, 0.34, 0.3, 0.21, 0.28, 0.25, 0.25, 0.25, 0.29, 0.24, 0.23, 0.27, 0.35, 0.26, 0.27, 0.25, 0.25, 0.29, 0.3, 0.28, 0.25, 0.28, 0.33, 0.29, 0.34, 0.27, 0.27, 0.24, 0.28, 0.28, 0.29, 0.31, 0.25, 0.32, 0.31, 0.2, 0.32, 0.19, 0.27, 0.28, 0.19, 0.25, 0.32, 0.3, 0.27, 0.29, 0.28, 0.25, 0.32, 0.26, 0.22, 0.26, 0.32, 0.31, 0.25, 0.28, 0.26, 0.25, 0.22, 0.26, 0.2, 0.25, 0.28, 0.22, 0.14, 0.28, 0.24, 0.32, 0.3, 0.16, 0.26, 0.28, 0.28, 0.23, 0.3, 0.26, 0.28, 0.27, 0.23, 0.27, 0.3, 0.28, 0.27, 0.29, 0.29, 0.24, 0.3, 0.2, 0.26, 0.33, 0.25, 0.31, 0.26, 0.29, 0.21, 0.22, 0.26, 0.2, 0.31, 0.23, 0.25, 0.29, 0.19, 0.27, 0.25, 0.25, 0.27, 0.22, 0.22, 0.26, 0.28, 0.29, 0.23, 0.2, 0.24, 0.25, 0.23, 0.25, 0.28, 0.3, 0.24, 0.24, 0.3, 0.24, 0.26, 0.33, 0.21, 0.32, 0.31, 0.28, 0.28, 0.25, 0.3, 0.26, 0.26, 0.27, 0.35, 0.19, 0.3, 0.25, 0.23, 0.23, 0.24, 0.28, 0.28, 0.3, 0.23, 0.23, 0.24, 0.27, 0.24, 0.26, 0.21, 0.26, 0.26, 0.28, 0.29, 0.28, 0.27, 0.22, 0.26, 0.3, 0.33, 0.26, 0.23, 0.34, 0.28, 0.3, 0.29, 0.26, 0.24, 0.24, 0.33, 0.24, 0.25, 0.31, 0.21, 0.28, 0.23, 0.27, 0.3, 0.21, 0.22, 0.33, 0.26, 0.16, 0.24, 0.29, 0.28, 0.24, 0.26, 0.3, 0.18, 0.32, 0.25, 0.29, 0.26, 0.24, 0.23, 0.29, 0.3, 0.3, 0.24, 0.24, 0.32, 0.29, 0.17, 0.28, 0.26, 0.28, 0.29, 0.27, 0.32, 0.29, 0.24, 0.25, 0.32, 0.32, 0.24, 0.28, 0.28, 0.3, 0.27, 0.29, 0.3, 0.2, 0.24, 0.28, 0.29, 0.25, 0.28, 0.2, 0.23, 0.27, 0.2, 0.28, 0.23, 0.29, 0.25, 0.21, 0.32, 0.24, 0.24, 0.3, 0.32, 0.34, 0.24, 0.25, 0.26, 0.18, 0.23, 0.29, 0.22, 0.21, 0.32, 0.26, 0.3, 0.32, 0.25, 0.25, 0.3, 0.34, 0.22, 0.26, 0.32, 0.28, 0.17, 0.29, 0.28, 0.25, 0.28, 0.28, 0.26, 0.32, 0.26, 0.27, 0.27, 0.29, 0.29, 0.26, 0.26, 0.24, 0.22, 0.28, 0.25, 0.28, 0.23, 0.22, 0.32, 0.29, 0.25, 0.27, 0.35, 0.3, 0.31, 0.26, 0.25, 0.21, 0.24, 0.3, 0.23, 0.3, 0.22, 0.31, 0.25, 0.19, 0.22, 0.24, 0.29, 0.3, 0.26, 0.26, 0.2, 0.2, 0.3, 0.25, 0.28, 0.29, 0.22, 0.24, 0.28, 0.3, 0.22, 0.27, 0.28, 0.24, 0.27, 0.27, 0.28, 0.26, 0.34, 0.31, 0.21, 0.25, 0.23, 0.26, 0.29, 0.25, 0.29, 0.31, 0.24, 0.25, 0.25, 0.26, 0.25, 0.23, 0.18, 0.29, 0.31, 0.31, 0.26, 0.27, 0.26, 0.3, 0.33, 0.26, 0.22, 0.28, 0.27, 0.32, 0.33, 0.26, 0.22, 0.25, 0.2, 0.29, 0.23, 0.28, 0.27, 0.28, 0.31, 0.25, 0.26, 0.19, 0.29, 0.23, 0.26, 0.3, 0.3, 0.29, 0.3, 0.23, 0.28, 0.29, 0.32, 0.23, 0.31, 0.33, 0.24, 0.24, 0.16, 0.3, 0.29, 0.22, 0.22, 0.27, 0.21, 0.18, 0.24, 0.22, 0.3, 0.26, 0.22, 0.36, 0.28, 0.28, 0.3, 0.27, 0.26, 0.27, 0.34, 0.26, 0.21, 0.27, 0.16, 0.28, 0.25, 0.25, 0.23, 0.17, 0.25, 0.34, 0.25, 0.31, 0.34, 0.27, 0.32, 0.24, 0.29, 0.25, 0.26, 0.22, 0.28, 0.27, 0.22, 0.27, 0.26, 0.26, 0.34, 0.25, 0.27, 0.28, 0.29, 0.23, 0.29, 0.22, 0.28, 0.3, 0.28, 0.24, 0.27, 0.28, 0.27, 0.24, 0.31, 0.26, 0.25, 0.23, 0.26, 0.31, 0.26, 0.27, 0.24, 0.27, 0.3, 0.27, 0.3, 0.27, 0.27, 0.21, 0.25, 0.28, 0.25, 0.32, 0.27, 0.31, 0.29, 0.22, 0.3, 0.27, 0.23, 0.31, 0.25, 0.28, 0.29, 0.21, 0.19, 0.23, 0.22, 0.27, 0.23, 0.21, 0.28, 0.27, 0.3, 0.24, 0.2, 0.24, 0.3, 0.25, 0.29, 0.27, 0.27, 0.25, 0.24, 0.26, 0.26, 0.28, 0.32, 0.32, 0.25, 0.27, 0.22, 0.21, 0.24, 0.25, 0.25, 0.31, 0.21, 0.23, 0.27, 0.26, 0.28, 0.28, 0.29, 0.24, 0.22, 0.29, 0.26, 0.24, 0.29, 0.2, 0.28, 0.2, 0.27, 0.28, 0.29, 0.22, 0.34, 0.23, 0.21, 0.28, 0.27, 0.32, 0.24, 0.23, 0.23, 0.21, 0.25, 0.21, 0.23, 0.27, 0.26, 0.27, 0.35, 0.28, 0.26, 0.31, 0.29, 0.23, 0.29, 0.21, 0.27, 0.26, 0.24, 0.24, 0.3, 0.3, 0.33, 0.29, 0.29, 0.29, 0.25, 0.28, 0.37, 0.28, 0.29, 0.18, 0.26, 0.32, 0.3, 0.25, 0.2, 0.28, 0.25, 0.25, 0.15, 0.37, 0.28, 0.23, 0.29, 0.24, 0.26, 0.29, 0.27, 0.29, 0.27, 0.24, 0.18, 0.29, 0.25, 0.3, 0.28, 0.31, 0.28, 0.24, 0.31, 0.25, 0.22, 0.21, 0.21, 0.26, 0.35, 0.29, 0.31, 0.25, 0.21, 0.28, 0.19, 0.25, 0.35, 0.27, 0.24, 0.27, 0.28, 0.19, 0.25, 0.24, 0.22, 0.3, 0.32, 0.28, 0.28, 0.3, 0.23, 0.31, 0.22, 0.22, 0.29, 0.25, 0.29, 0.23, 0.25, 0.28, 0.33, 0.25, 0.31, 0.22, 0.2, 0.25, 0.31, 0.29, 0.24, 0.27, 0.25, 0.29, 0.23, 0.29, 0.28, 0.25, 0.29, 0.23, 0.34, 0.27, 0.29, 0.22, 0.28, 0.33, 0.22, 0.39, 0.26, 0.24, 0.19, 0.27, 0.22, 0.2, 0.31, 0.27, 0.2, 0.23, 0.28, 0.2, 0.3, 0.21, 0.29, 0.26, 0.25, 0.31, 0.23, 0.31, 0.23, 0.29, 0.29, 0.26, 0.33, 0.29, 0.27, 0.22, 0.27, 0.28, 0.23, 0.22, 0.23, 0.21, 0.32, 0.19, 0.36, 0.24, 0.26, 0.28, 0.23, 0.23, 0.33, 0.3, 0.29, 0.27, 0.28, 0.28, 0.22, 0.33, 0.29, 0.26, 0.31, 0.24, 0.25, 0.27, 0.26, 0.23, 0.24, 0.18, 0.3, 0.28, 0.25, 0.23, 0.32, 0.29, 0.25, 0.31, 0.26, 0.18, 0.3, 0.28, 0.2, 0.23, 0.25, 0.25, 0.27, 0.26, 0.26, 0.24, 0.22, 0.31, 0.26, 0.29, 0.35, 0.21, 0.31, 0.21, 0.24, 0.23, 0.24, 0.24, 0.28, 0.23, 0.25, 0.18, 0.26, 0.29, 0.26, 0.26, 0.21, 0.27, 0.32, 0.24, 0.26, 0.25, 0.24, 0.23, 0.23, 0.26, 0.26, 0.3, 0.23, 0.22, 0.25, 0.33, 0.26, 0.19, 0.32, 0.29, 0.25, 0.33, 0.32, 0.24, 0.28, 0.3, 0.25, 0.27, 0.33, 0.31, 0.26, 0.25, 0.21, 0.35, 0.26, 0.24, 0.28, 0.27, 0.32, 0.28, 0.33, 0.27, 0.23, 0.29, 0.26, 0.28, 0.2, 0.2, 0.25, 0.34, 0.29, 0.21, 0.27, 0.37, 0.3, 0.24, 0.28, 0.27, 0.29, 0.32, 0.23, 0.28, 0.22, 0.29, 0.24, 0.26, 0.3, 0.3, 0.26, 0.24, 0.23, 0.24, 0.26, 0.28, 0.24, 0.32, 0.23, 0.21, 0.3, 0.22, 0.28, 0.34, 0.23, 0.29, 0.25, 0.21, 0.26, 0.33, 0.23, 0.3, 0.25, 0.24, 0.25, 0.22, 0.26, 0.26, 0.33, 0.3, 0.29, 0.3, 0.23, 0.29, 0.21, 0.28, 0.26, 0.3, 0.27, 0.28, 0.2, 0.33, 0.25, 0.24, 0.26, 0.23, 0.31, 0.22, 0.27, 0.2, 0.29, 0.25, 0.29, 0.25, 0.37, 0.23, 0.25, 0.3, 0.27, 0.31, 0.27, 0.19, 0.34, 0.26, 0.21, 0.37, 0.36, 0.23, 0.24, 0.21, 0.28, 0.19, 0.28, 0.23, 0.25, 0.31, 0.24, 0.19, 0.29, 0.22, 0.27, 0.26, 0.26, 0.31, 0.27, 0.25, 0.26, 0.25, 0.23, 0.33, 0.31, 0.33, 0.3, 0.26, 0.31, 0.26, 0.27, 0.27, 0.33, 0.25, 0.27, 0.25, 0.31, 0.25, 0.24, 0.26, 0.26, 0.2, 0.33, 0.18, 0.26, 0.29, 0.24, 0.33, 0.29, 0.33, 0.28, 0.34, 0.24, 0.33, 0.26, 0.29, 0.25, 0.34, 0.29, 0.29, 0.29, 0.25, 0.21, 0.26, 0.24, 0.33, 0.28, 0.3, 0.3, 0.25, 0.21, 0.28, 0.27, 0.28, 0.25, 0.23, 0.29, 0.25, 0.25, 0.29, 0.25, 0.17, 0.26, 0.29, 0.32, 0.27, 0.27, 0.26, 0.3, 0.25, 0.24, 0.3, 0.25, 0.22, 0.25, 0.26, 0.28, 0.26, 0.32, 0.32, 0.31, 0.2, 0.28, 0.27, 0.29, 0.31, 0.27, 0.24, 0.28, 0.24, 0.25, 0.28, 0.25, 0.21, 0.23, 0.29, 0.3, 0.25, 0.22, 0.28, 0.33, 0.26, 0.3, 0.27, 0.26, 0.23, 0.3, 0.31, 0.3, 0.28, 0.25, 0.25, 0.29, 0.24, 0.31, 0.29, 0.28, 0.27, 0.26, 0.2, 0.25, 0.22, 0.25, 0.28, 0.29, 0.28, 0.28, 0.25, 0.21, 0.26, 0.35, 0.25, 0.21, 0.25, 0.28, 0.27, 0.22, 0.24, 0.29, 0.26, 0.27, 0.32, 0.22, 0.32]\n"
          ]
        }
      ],
      "source": [
        "#test\n",
        "from torchmetrics.classification import MulticlassAccuracy\n",
        "iterator = iter(test_dataloader)\n",
        "accuracy = MulticlassAccuracy(num_classes=72).to(gpu)\n",
        "softmax = torch.nn.Softmax(dim=1)\n",
        "accuracies = []\n",
        "for data, label in iterator:\n",
        "    #reshape data and label from sequence_len x num_classes\n",
        "    prediction = model(data)\n",
        "    prediction = torch.stack(prediction).reshape(100,72)\n",
        "    label = torch.stack(label).reshape(100,72)\n",
        "    #compute accuracy\n",
        "    pred_arg = torch.argmax(softmax(prediction),dim=1)\n",
        "    label_arg = torch.argmax(label,dim=1)\n",
        "    acc = sum(torch.eq(pred_arg, label_arg).tolist())/100\n",
        "    accuracies.append(acc)\n",
        "print(accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca5fda5-8841-4f0f-bacd-27f3c6ec365a",
      "metadata": {
        "id": "4ca5fda5-8841-4f0f-bacd-27f3c6ec365a",
        "outputId": "41d2f0d9-bb14-49f9-9292-4bb65a487acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean accuracy is: 0.2651143695014595\n"
          ]
        }
      ],
      "source": [
        "print(\"Mean accuracy is: \" + str(sum(accuracies)/len(accuracies)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0933354f-a73b-429c-8cd3-3a3e9935205e",
      "metadata": {
        "id": "0933354f-a73b-429c-8cd3-3a3e9935205e"
      },
      "outputs": [],
      "source": [
        "# this function encoded a string as an input fitting for the NN of size (SxN)\n",
        "# * N = number of classes\n",
        "# * S = length of the string (in terms of characters)\n",
        "def generate_phrase_to_NN_input(num_classes, dictionary, device):\n",
        "    # input: a string\n",
        "    # output: a Sx72 one-hot tensor\n",
        "    def phrase_to_NN_input(phrase):\n",
        "        result = []\n",
        "        for char in phrase:\n",
        "            word_converted = torch.zeros(num_classes)\n",
        "            word_index = dictionary.loc[dictionary['data'] == char]['int_encoding'].item()\n",
        "            word_converted[word_index] = 1\n",
        "            result.append(word_converted)\n",
        "        result = torch.stack(result)\n",
        "        result = result.to(device)\n",
        "        return result\n",
        "    return phrase_to_NN_input\n",
        "phrase_to_NN_input = generate_phrase_to_NN_input(72,dictionary, gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd1f5fd-6114-4788-9cbf-0bcf092dfd9f",
      "metadata": {
        "id": "edd1f5fd-6114-4788-9cbf-0bcf092dfd9f"
      },
      "outputs": [],
      "source": [
        "# this function decodes a string expressed as a tensor of shape SxN with values x\\in{0,1}, to a string of length S\n",
        "def generate_NN_output_to_phrase(num_classes, dictionary, device):\n",
        "    # input: a string\n",
        "    # output: a Sx72 one-hot tensor\n",
        "    def NN_output_to_phrase(nn_output):\n",
        "        result = \"\"\n",
        "        for tensor_hot_enc in nn_output:\n",
        "            idx_word = torch.argmax(tensor_hot_enc).item()\n",
        "            char_decoded = dictionary.loc[dictionary['int_encoding'] == idx_word]['data'].item()\n",
        "            result = result + char_decoded\n",
        "        return result\n",
        "    return NN_output_to_phrase\n",
        "NN_output_to_phrase = generate_NN_output_to_phrase(72,dictionary, gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a98ced-b50d-4cd0-a3fe-cc14c53b822b",
      "metadata": {
        "id": "f3a98ced-b50d-4cd0-a3fe-cc14c53b822b"
      },
      "outputs": [],
      "source": [
        "wordminusminus_encoded = phrase_to_NN_input(\"--\")\n",
        "assert wordminusminus_encoded[0][4].item() == 1.0 and wordminusminus_encoded[1][4].item() == 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4b5b946-7cdf-4443-bae4-effa8c835bb5",
      "metadata": {
        "id": "c4b5b946-7cdf-4443-bae4-effa8c835bb5"
      },
      "outputs": [],
      "source": [
        "wordminusminus_decoded = NN_output_to_phrase(wordminusminus_encoded)\n",
        "assert wordminusminus_decoded == \"--\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d76b5a50-34e5-4794-9671-e097ceefc607",
      "metadata": {
        "id": "d76b5a50-34e5-4794-9671-e097ceefc607",
        "outputId": "7662545a-0a69-4ac2-a1d6-607f383b836e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ua ua ua                                  \n"
          ]
        }
      ],
      "source": [
        "phrase = \"eqweqweqwaaIII--------IIIIIaaaaaAAAAAAAAAAA\"\n",
        "phrase_encoded = phrase_to_NN_input(phrase)\n",
        "prediction = model([phrase_encoded])\n",
        "prediction_decoded = NN_output_to_phrase(prediction[0])\n",
        "print(prediction_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3a4c19-f51e-4128-ac7a-c7a56707889d",
      "metadata": {
        "id": "1b3a4c19-f51e-4128-ac7a-c7a56707889d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba586a9-53eb-4910-9176-5a78834666c9",
      "metadata": {
        "id": "8ba586a9-53eb-4910-9176-5a78834666c9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3052c83b-65ec-4f6f-8f4d-9bf929d961f1",
      "metadata": {
        "id": "3052c83b-65ec-4f6f-8f4d-9bf929d961f1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f29abaef-4933-4993-81c1-f09fcd7bd245",
      "metadata": {
        "id": "f29abaef-4933-4993-81c1-f09fcd7bd245"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "711424b6-f454-4104-a5d9-4aaf07dff976",
      "metadata": {
        "id": "711424b6-f454-4104-a5d9-4aaf07dff976",
        "outputId": "892e919d-004c-478f-cdb0-0662a99151b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.08"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "iterator = iter(test_dataloader)\n",
        "data, label = next(iterator)\n",
        "prediction = torch.stack(model(data))\n",
        "data_arg = torch.argmax(softmax(prediction).reshape(100,72),dim=1)\n",
        "label_arg = torch.argmax(torch.stack(label).reshape(100,72),dim=1)\n",
        "sum(torch.eq(data_arg, label_arg).tolist())/100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d896b35-c1df-4e11-ba67-e447cc56a858",
      "metadata": {
        "id": "1d896b35-c1df-4e11-ba67-e447cc56a858"
      },
      "outputs": [],
      "source": [
        "37.65237474441528"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4747ae41-9ad7-445a-aff3-16cf86eee6b4",
      "metadata": {
        "id": "4747ae41-9ad7-445a-aff3-16cf86eee6b4",
        "outputId": "c71d8343-8839-4eab-cddc-d9e9f8dbedb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 2],\n",
            "        [2, 3]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([[1,2],[2,3]])\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7570f972-8951-44e7-9957-1e7dfd78df77",
      "metadata": {
        "id": "7570f972-8951-44e7-9957-1e7dfd78df77"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "a = torch.tensor([2.0], requires_grad=True)\n",
        "b = torch.tensor([3.0], requires_grad=True)\n",
        "\n",
        "c = 0\n",
        "for i in range(3):\n",
        "    c = c + a\n",
        "    print(c)\n",
        "\n",
        "c = c + torch.log(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8684f5bc-1489-40ad-8cc2-860c279bb1fe",
      "metadata": {
        "id": "8684f5bc-1489-40ad-8cc2-860c279bb1fe"
      },
      "outputs": [],
      "source": [
        "c.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09bf2ee8-73f0-4f14-8600-1c8717c2aff6",
      "metadata": {
        "id": "09bf2ee8-73f0-4f14-8600-1c8717c2aff6",
        "outputId": "31c140af-cac8-4ef3-c017-ece653cadd86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3.])\n",
            "tensor([0.3333])\n"
          ]
        }
      ],
      "source": [
        "print(a.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2664bb-648f-4fd6-828e-a0662835455e",
      "metadata": {
        "id": "bb2664bb-648f-4fd6-828e-a0662835455e",
        "outputId": "4b544122-4639-45fa-ec42-40cda06f9961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(2.0, shape=(), dtype=float32)\n",
            "tf.Tensor(4.0, shape=(), dtype=float32)\n",
            "tf.Tensor(6.0, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "a = tf.Variable(2.0)\n",
        "b = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    c = 0\n",
        "    for i in range(3):\n",
        "        c = c + a\n",
        "        print(c)\n",
        "\n",
        "    c = c + tf.math.log(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e551091d-c108-4f86-a501-8edb76297de4",
      "metadata": {
        "id": "e551091d-c108-4f86-a501-8edb76297de4",
        "outputId": "dea7ed31-bc86-418c-acb4-28f1298e1883"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=3.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=0.33333334>]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dy_da = tape.gradient(c, [a,b])\n",
        "dy_da"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f71d4ea-1743-4598-81db-d75c13eb932e",
      "metadata": {
        "id": "9f71d4ea-1743-4598-81db-d75c13eb932e",
        "outputId": "d2b712bb-b45e-4952-fe32-7d799a9d1fa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(7.0986123, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d268f1f2-31a2-4615-b5b9-0226ac2dbd78",
      "metadata": {
        "id": "d268f1f2-31a2-4615-b5b9-0226ac2dbd78",
        "outputId": "3c858739-935e-4408-a4de-4e837e10b290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  data  int_encoding\n",
            "0    a             0\n",
            "1    b             1\n",
            "2    c             2\n",
            "3    c             2\n",
            "4    a             0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "data = ('a','b','c','c','a')\n",
        "dictionary = pd.DataFrame(data, columns=['data'])\n",
        "\n",
        "dictionary['int_encoding'] = labelencoder.fit_transform(dictionary['data'])\n",
        "print(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc175409-57af-405c-ad38-7067c4328edf",
      "metadata": {
        "id": "cc175409-57af-405c-ad38-7067c4328edf",
        "outputId": "e675c743-b099-40f1-a51e-a006f62f979b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   int_encoding\n",
            "0             0\n",
            "1             1\n",
            "2             2\n",
            "3             2\n",
            "4             0\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "one_hot_enc = OneHotEncoder()\n",
        "one_hot = one_hot_enc.fit_transform(dictionary[['int_encoding']]).toarray()\n",
        "print(dictionary[['int_encoding']])\n",
        "print(one_hot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be69b90-b292-45d3-99f6-a44e8958b7d8",
      "metadata": {
        "id": "2be69b90-b292-45d3-99f6-a44e8958b7d8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "mat = torch.empty(5,100).to(gpu)\n",
        "for i in range(0, 1000000):\n",
        "    mat = torch.mm(mat, torch.transpose(mat,0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c79497b9-91d0-4a01-9182-890cf48bd002",
      "metadata": {
        "id": "c79497b9-91d0-4a01-9182-890cf48bd002",
        "outputId": "c48c99ae-d277-41fe-86fb-d1e78a7a1f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[6.6648e-10, 1.1040e-05, 2.6081e+20],\n",
            "        [2.0975e-07, 2.1876e-04, 4.3921e-05]])\n",
            "tensor([[6.6648e-10, 2.0975e-07],\n",
            "        [1.1040e-05, 2.1876e-04],\n",
            "        [2.6081e+20, 4.3921e-05]])\n",
            "tensor([[       inf, 1.1455e+16],\n",
            "        [1.1455e+16, 4.9784e-08]])\n"
          ]
        }
      ],
      "source": [
        "mat = torch.empty(2,3)\n",
        "a = torch.transpose(mat,0,1)\n",
        "print(mat)\n",
        "print(a)\n",
        "print(torch.mm(mat,a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb623997-50f2-4101-b5a2-868ad99c05f8",
      "metadata": {
        "id": "fb623997-50f2-4101-b5a2-868ad99c05f8",
        "outputId": "d6287941-d8cd-46db-ffd2-0f1b78a829b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.arange(10).reshape(5,2)\n",
        "a\n",
        "torch.split(a,1)\n",
        "a.size(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b437d3c0-c0af-4056-9f97-df3f1b08f17f",
      "metadata": {
        "id": "b437d3c0-c0af-4056-9f97-df3f1b08f17f",
        "outputId": "469bd805-5953-4150-95ce-3430ed8b4b5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1, 2])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.1867, -0.5635]],\n",
              "\n",
              "        [[-0.5525, -0.4179]]])"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.stack([torch.randn((1,2)),torch.randn((1,2))])\n",
        "print(a.size())\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae95d04e-70c2-43cb-a1e1-d779812b9e3e",
      "metadata": {
        "tags": [],
        "id": "ae95d04e-70c2-43cb-a1e1-d779812b9e3e",
        "outputId": "522172d5-b5b0-472b-f97b-d0fe816a778a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "128\n",
            "72\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [97], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(b\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(b\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n",
            "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
          ]
        }
      ],
      "source": [
        "a = next(iter(train_dataloader))\n",
        "b = torch.stack(a[0])\n",
        "print(b.size(0))\n",
        "print(b.size(1))\n",
        "print(b.size(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b26756-03eb-4d3e-be09-e191cebf1602",
      "metadata": {
        "id": "81b26756-03eb-4d3e-be09-e191cebf1602",
        "outputId": "fa14ea4a-a67f-4383-f858-1835acf64932"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'softmax'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [91], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m pred \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mFloatTensor([\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m20\u001b[39m]),torch\u001b[38;5;241m.\u001b[39mFloatTensor([\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m])]\n\u001b[0;32m      4\u001b[0m softmax \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([[\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],softmax(pred)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]],[softmax(pred)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],softmax(pred)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]]])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m      7\u001b[0m loss(pred,label)\n",
            "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
            "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1376\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1834\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1832\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1834\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m(dim)\n\u001b[0;32m   1835\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1836\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'softmax'"
          ]
        }
      ],
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "# pred=(a,c),(a,a), label=(a,b),(c,c)\n",
        "pred = [torch.FloatTensor([10,20]),torch.FloatTensor([20,10])\n",
        "softmax = torch.nn.Softmax(dim=0)\n",
        "label = torch.FloatTensor([[softmax(pred)[0][0],softmax(pred)[0][1]],[softmax(pred)[1][0],softmax(pred)[1][1]]])\n",
        "print(pred.size())\n",
        "loss(pred,label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0d1d66-d815-4641-8394-f61d51f43974",
      "metadata": {
        "id": "9d0d1d66-d815-4641-8394-f61d51f43974",
        "outputId": "b9fffe94-b429-4274-955e-41ee828e2a5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([6.])"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.ones(1, requires_grad=True)\n",
        "y = x**2\n",
        "z = x**3\n",
        "w = x**3\n",
        "z.backward()\n",
        "w.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e53b4bbf-6021-4c27-b87c-d7b797353bb7",
      "metadata": {
        "id": "e53b4bbf-6021-4c27-b87c-d7b797353bb7",
        "outputId": "4980824a-b114-42b4-c969-00609af7bef1"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m target \u001b[38;5;241m=\u001b[39m tensor([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m      3\u001b[0m preds \u001b[38;5;241m=\u001b[39m tensor([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtorchmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmulticlass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m accuracy(preds, target)\n",
            "File \u001b[1;32mD:\\Programmi\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchmetrics\\classification\\accuracy.py:490\u001b[0m, in \u001b[0;36mAccuracy.__new__\u001b[1;34m(cls, threshold, num_classes, average, mdmc_average, ignore_index, top_k, multiclass, subset_accuracy, task, num_labels, multidim_average, validate_args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_classes, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(top_k, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MulticlassAccuracy(num_classes, top_k, average, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch import tensor\n",
        "target = tensor([0, 1, 2, 3])\n",
        "preds = tensor([0, 2, 1, 3])\n",
        "accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=4)\n",
        "accuracy(preds, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a94f05d-8a65-4e46-af34-13c6c7de5229",
      "metadata": {
        "id": "4a94f05d-8a65-4e46-af34-13c6c7de5229"
      },
      "outputs": [],
      "source": [
        "y = torch.tensor([\n",
        "     [\n",
        "       [1, 0, 0],\n",
        "       [1, 0, 0]\n",
        "     ],\n",
        "     [\n",
        "       [0, 1, 0],\n",
        "       [0, 0, 1]\n",
        "     ],\n",
        "     [\n",
        "       [0, 0, 1],\n",
        "       [0, 0, 1]\n",
        "     ]\n",
        "   ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73572edf-58fa-4377-a1de-63edd46e23ce",
      "metadata": {
        "id": "73572edf-58fa-4377-a1de-63edd46e23ce",
        "outputId": "c8a10bf4-d5d8-4182-8d81-4171efef54f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 3])"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f3b11e1-caf8-4666-adb9-c5edb2444d78",
      "metadata": {
        "id": "9f3b11e1-caf8-4666-adb9-c5edb2444d78",
        "outputId": "77909d11-37a3-4355-93fa-a09f5a467217"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 0],\n",
              "        [1, 2],\n",
              "        [2, 2]])"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.argmax(y.float(),dim=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41635f7-8e0e-4a9b-a393-ba0ae1f05974",
      "metadata": {
        "id": "a41635f7-8e0e-4a9b-a393-ba0ae1f05974",
        "outputId": "a2cb2127-90c3-4c68-bc5b-2128f6827f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        data  int_encoding\n",
            "0          b            39\n",
            "1          e            42\n",
            "2          n            51\n",
            "4                        1\n",
            "6          u            58\n",
            "...      ...           ...\n",
            "1099082    -             4\n",
            "1099618    U            33\n",
            "1100751    Ãˆ            64\n",
            "1101278    N            26\n",
            "1104174    V            34\n",
            "\n",
            "[72 rows x 2 columns]\n",
            "72\n",
            "39\n"
          ]
        }
      ],
      "source": [
        "print(dictionary)\n",
        "print(len(dictionary))\n",
        "a = dictionary.loc[dictionary['data'] == 'b']['int_encoding']\n",
        "print(a[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d88a0ea-e6fc-40f2-8f7f-180d66d65557",
      "metadata": {
        "id": "1d88a0ea-e6fc-40f2-8f7f-180d66d65557"
      },
      "source": [
        "##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b017161f-557e-4242-b603-f587262a76d3",
      "metadata": {
        "id": "b017161f-557e-4242-b603-f587262a76d3"
      },
      "outputs": [],
      "source": [
        "fdsfdsfsdfsdfsdfsdfdsfs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}